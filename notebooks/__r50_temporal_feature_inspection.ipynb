{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import ast\n",
    "import mmcv\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from torchmetrics.functional.classification import (\n",
    "    multilabel_f1_score,\n",
    "    multilabel_precision,\n",
    "    multilabel_recall,\n",
    ")\n",
    "\n",
    "from data_utils import results2df\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Slowfast imports\n",
    "from slowfast.models import build_model\n",
    "from slowfast.utils.parser import load_config, alt_parse_args\n",
    "from slowfast.datasets.loader import construct_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating few-shot annotation file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/home/dl18206/Desktop/phd/code/personal/facebook/slowfast/dataset/annotations/standard/fg_only/standard/train.csv\"\n",
    "val_path = \"/home/dl18206/Desktop/phd/code/personal/facebook/slowfast/dataset/annotations/standard/fg_only/standard/val.csv\"\n",
    "metadata_path = \"/home/dl18206/Desktop/phd/code/personal/facebook/slowfast/dataset/metadata/with_negative_pairing/new_metadata.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "val_df = pd.read_csv(val_path)\n",
    "\n",
    "train_df.columns = [\"subject_id\", \"label\"]\n",
    "val_df.columns = [\"subject_id\", \"label\"]\n",
    "\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "with open(\n",
    "    \"/home/dl18206/Desktop/phd/code/personal/facebook/slowfast/dataset/metadata/behaviours.txt\",\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    behaviours = [beh.decode(\"utf-8\").strip() for beh in f.readlines()]\n",
    "\n",
    "train_df = train_df.merge(\n",
    "    metadata[[\"subject_id_fg\", \"value\"]], right_on=\"subject_id_fg\", left_on=\"subject_id\"\n",
    ")\n",
    "\n",
    "val_df = val_df.merge(\n",
    "    metadata[[\"subject_id_fg\", \"value\"]], right_on=\"subject_id_fg\", left_on=\"subject_id\"\n",
    ")\n",
    "\n",
    "\n",
    "def is_fs(x):\n",
    "    fs_behaviours = [\"aggression\"]\n",
    "    for b in fs_behaviours:\n",
    "        if b == x:\n",
    "            return True\n",
    "    for b in fs_behaviours:\n",
    "        if b in x.split(\",\"):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "train_df[\n",
    "    (train_df[\"value\"].apply(is_fs)) & (train_df.subject_id.str.startswith(\"acp\"))\n",
    "][[\"subject_id\", \"label\", \"value\"]]\n",
    "\n",
    "val_df[(val_df[\"value\"].apply(is_fs))][[\"subject_id\", \"label\", \"value\"]]\n",
    "\n",
    "# .to_csv(\n",
    "#     \"/home/dl18206/Desktop/phd/code/personal/facebook/slowfast/dataset/annotations/standard/fg_only/few_shot/aggression/val_aggression.csv\",\n",
    "#     index=False,\n",
    "#     header=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_neg = pd.read_csv(\n",
    "    \"/home/dl18206/Desktop/phd/code/personal/facebook/slowfast/dataset/annotations/standard/with_negative_pairing/standard/val.csv\"\n",
    ")\n",
    "val_df_neg.columns = [\"fg\", \"bg\", \"label\", \"negative\", \"utm_code\"]\n",
    "val_df_neg = val_df_neg.merge(\n",
    "    metadata[[\"subject_id_fg\", \"value\"]], right_on=\"subject_id_fg\", left_on=\"fg\"\n",
    ")[[\"fg\", \"bg\", \"label\", \"value\"]]\n",
    "\n",
    "fs_val_df_neg = val_df_neg[val_df_neg[\"value\"].apply(is_fs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_labels = fs_val_df_neg.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_val_df_neg[[\"bg\", \"label\"]].to_csv(\n",
    "    \"/home/dl18206/Desktop/phd/code/personal/facebook/slowfast/dataset/annotations/standard/fg_only/few_shot/background/val.csv\",\n",
    "    index=False,\n",
    "    header=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Temporal activation maps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "path_to_config = \"/home/dl18206/Desktop/phd/code/personal/facebook/slowfast/configs/SLOW_8x8_R50_Local.yaml\"\n",
    "path_to_ckpt = \"/home/dl18206/Desktop/phd/code/personal/facebook/slowfast/checkpoint_epoch_00100.pyth\"\n",
    "\n",
    "args = alt_parse_args()[:-1]\n",
    "cfg = load_config(\n",
    "    args[0],\n",
    "    path_to_config=path_to_config,\n",
    ")\n",
    "checkpoint = torch.load(path_to_ckpt, map_location=\"cpu\")\n",
    "\n",
    "model = build_model(cfg)\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "model.eval()\n",
    "model.cpu()\n",
    "\n",
    "classifier = model.head.projection\n",
    "\n",
    "# Load the data\n",
    "loader = construct_loader(cfg, \"test\")  # dataset = build_dataset(\"tap\", cfg, \"train\")\n",
    "inputs, labels, index, time, meta = next(iter(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funcs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_map(model, sample):\n",
    "    with torch.no_grad():\n",
    "        feature_map = model.s5(\n",
    "            model.s4(model.s3(model.s2(model.s1([sample.unsqueeze(0)]))))\n",
    "        )[\n",
    "            0\n",
    "        ]  # TODO: Investigate features at earlier layers\n",
    "    return feature_map\n",
    "\n",
    "\n",
    "def extract_frame_wise_features(feature_map, t):\n",
    "    spatially_pooled = F.adaptive_avg_pool3d(feature_map, (t, 1, 1))\n",
    "    frame_wise_features = torch.flatten(spatially_pooled, start_dim=2)\n",
    "    return frame_wise_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name, feat_map = [], []\n",
    "for i, (input, label, idx, time, meta) in tqdm(enumerate(loader)):\n",
    "    feature_map = get_feature_map(model, input[0][0])\n",
    "    name.append(meta[\"video_name\"])\n",
    "    feat_map.append(feature_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 0\n",
    "label = labels[sample_index]\n",
    "sample = inputs[0][sample_index]\n",
    "negative_sample = inputs[0][-1]\n",
    "\n",
    "# sample = sample[\n",
    "#     :,\n",
    "#     :,\n",
    "#     0:224,\n",
    "# ] # TODO: Investigate timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = get_feature_map(model, sample)\n",
    "negative_feature_map = get_feature_map(model, negative_sample)\n",
    "print(f\"Feature map shape: {feature_map.shape}\")\n",
    "print(f\"Video-level map shape: {F.adaptive_avg_pool3d(feature_map, (1, 1, 1)).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "framewise_features = extract_frame_wise_features(feature_map, t=16)\n",
    "negative_framewise_features = extract_frame_wise_features(negative_feature_map, t=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute frame-wise cosine similarity\n",
    "normalise = True\n",
    "framewise_features = framewise_features.squeeze(0)\n",
    "framewise_features = framewise_features.T\n",
    "if normalise:\n",
    "    framewise_features = F.normalize(framewise_features, p=2, dim=1)\n",
    "print(framewise_features.shape, framewise_features.T.shape)\n",
    "framewise_cosine_similarity = torch.mm(framewise_features, framewise_features.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute frame-wise cosine similarity between the sample and the negative sample\n",
    "negative_framewise_features = negative_framewise_features.squeeze(0)\n",
    "negative_framewise_features = negative_framewise_features.T\n",
    "\n",
    "if normalise:\n",
    "    negative_framewise_features = F.normalize(negative_framewise_features, p=2, dim=1)\n",
    "\n",
    "negative_framewise_cosine_similarity = torch.mm(\n",
    "    framewise_features, negative_framewise_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear projection into lower dimensional space\n",
    "\n",
    "linear_projection = torch.nn.Linear(\n",
    "    in_features=framewise_features.shape[1], out_features=128\n",
    ")\n",
    "\n",
    "reduced_framewise_features = linear_projection(framewise_features)\n",
    "\n",
    "if normalise:\n",
    "    reduced_framewise_features = F.normalize(reduced_framewise_features, p=2, dim=1)\n",
    "\n",
    "reduced_framewise_cosine_similarity = torch.mm(\n",
    "    reduced_framewise_features, reduced_framewise_features.T\n",
    ")\n",
    "\n",
    "# Plot frame-wise cosine similarity\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(\n",
    "    reduced_framewise_cosine_similarity.detach().numpy(),\n",
    "    xticklabels=False,\n",
    "    yticklabels=False,\n",
    "    cmap=\"viridis\",\n",
    "    annot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_wise_logits = []\n",
    "for feat in framewise_features:\n",
    "    frame_wise_logits.append(torch.sigmoid(classifier(feat).detach()).numpy())\n",
    "frame_wise_logits = np.array(frame_wise_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video feats\n",
    "video_level_features = F.adaptive_avg_pool3d(feature_map, (1, 1, 1))\n",
    "video_level_features = torch.flatten(video_level_features, start_dim=1)\n",
    "\n",
    "# Video logits\n",
    "video_level_logits = torch.sigmoid(classifier(video_level_features)).detach().numpy()\n",
    "video_level_logits = np.array(video_level_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cosine similarity matrix\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(framewise_cosine_similarity.detach().numpy(), cmap=\"viridis\", annot=True)\n",
    "plt.title(\"Frame-wise cosine similarity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot frame-wise logits as heatmap\n",
    "plt.figure(figsize=(12, 8), dpi=100)\n",
    "sns.heatmap(frame_wise_logits.T, cmap=\"viridis\", annot=True, yticklabels=behaviours)\n",
    "plt.title(\"Frame-wise logits\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot framewise logits and video-level logits side by side\n",
    "fig, ax = plt.subplots(\n",
    "    1, 3, figsize=(20, 10), gridspec_kw={\"width_ratios\": [3, 0.75, 0.75]}\n",
    ")\n",
    "sns.heatmap(\n",
    "    frame_wise_logits.T, cmap=\"viridis\", annot=True, yticklabels=behaviours, ax=ax[0]\n",
    ")\n",
    "sns.heatmap(\n",
    "    video_level_logits.T,\n",
    "    cmap=\"viridis\",\n",
    "    annot=True,\n",
    "    ax=ax[1],\n",
    "    cbar=False,\n",
    ")\n",
    "sns.heatmap(\n",
    "    label.unsqueeze(0).numpy().T,\n",
    "    cmap=\"viridis\",\n",
    "    annot=True,\n",
    "    ax=ax[2],\n",
    "    cbar=False,\n",
    ")\n",
    "\n",
    "ax[0].title.set_text(\"Masked Frame-wise Logits\")\n",
    "ax[1].title.set_text(\"Video-level Logits\")\n",
    "ax[2].title.set_text(\"Label\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "subframe_features = framewise_features[0:8:,]\n",
    "\n",
    "# Subframe logits\n",
    "subframe_logits = []\n",
    "for feat in subframe_features:\n",
    "    subframe_logits.append(torch.sigmoid(classifier(feat).detach()).numpy())\n",
    "\n",
    "subframe_logits = np.array(subframe_logits)\n",
    "\n",
    "# Adaptive pool over subframe features\n",
    "subframe_features = subframe_features.unsqueeze(0)\n",
    "subframe_features = F.adaptive_avg_pool2d(subframe_features, (1, 2048))[0]\n",
    "\n",
    "# Apply classifier to subframe features\n",
    "subvideo_logits = torch.sigmoid(classifier(subframe_features)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot framewise logits and video-level logits side by side\n",
    "fig, ax = plt.subplots(\n",
    "    1, 4, figsize=(20, 10), gridspec_kw={\"width_ratios\": [3, 0.75, 0.75, 0.75]}\n",
    ")\n",
    "sns.heatmap(\n",
    "    subframe_logits.T, cmap=\"viridis\", annot=True, yticklabels=behaviours, ax=ax[0]\n",
    ")\n",
    "sns.heatmap(\n",
    "    subvideo_logits.T,\n",
    "    cmap=\"viridis\",\n",
    "    annot=True,\n",
    "    ax=ax[1],\n",
    "    cbar=False,\n",
    ")\n",
    "\n",
    "sns.heatmap(\n",
    "    video_level_logits.T,\n",
    "    cmap=\"viridis\",\n",
    "    annot=True,\n",
    "    ax=ax[2],\n",
    "    cbar=False,\n",
    ")\n",
    "\n",
    "sns.heatmap(\n",
    "    label.unsqueeze(0).numpy().T,\n",
    "    cmap=\"viridis\",\n",
    "    annot=True,\n",
    "    ax=ax[3],\n",
    "    cbar=False,\n",
    ")\n",
    "\n",
    "ax[0].title.set_text(\"Masked Frame-wise Logits\")\n",
    "ax[1].title.set_text(\"Sub-video Logits\")\n",
    "ax[2].title.set_text(\"Video-level Logits\")\n",
    "ax[3].title.set_text(\"Label\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = rearrange(sample, \"c t w h -> t c w h\")\n",
    "\n",
    "\n",
    "def plot_video(clip, nrow=8):\n",
    "    grid = make_grid(clip, nrow=nrow)\n",
    "    img = torchvision.transforms.ToPILImage()(grid)\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_video(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_head_attention = torch.nn.MultiheadAttention(\n",
    "    embed_dim=2048, num_heads=1, batch_first=True\n",
    ")\n",
    "q = framewise_features[\n",
    "    14:15,\n",
    "    :,\n",
    "].unsqueeze(0)\n",
    "kv = framewise_features.unsqueeze(0)\n",
    "output, attention_weights = multi_head_attention(q, kv, kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q.shape, kv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(\n",
    "    attention_weights.squeeze(0).T.detach().numpy(),\n",
    "    cmap=\"viridis\",\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cbar=True,\n",
    ")\n",
    "plt.title(\"Attention Weights Visualization\")\n",
    "plt.xlabel(\"Query\")\n",
    "plt.ylabel(\"Sequence Position\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_framewise_features.squeeze_(0).shape, framewise_features.squeeze_(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_framewise_features.shape, framewise_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_framewise_features = negative_framewise_features.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine sim for temporal masking\n",
    "background_embedding = framewise_features[\n",
    "    12:13:, :\n",
    "]  # negative_framewise_features[10:11:, :]\n",
    "\n",
    "cosine_sim = cosine_similarity(\n",
    "    background_embedding.detach().numpy(), framewise_features.detach().numpy()\n",
    ")\n",
    "\n",
    "# Normalize cosine similarity between 0 and 1\n",
    "cosine_sim = (cosine_sim - np.min(cosine_sim)) / (\n",
    "    np.max(cosine_sim) - np.min(cosine_sim)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 2))\n",
    "sns.heatmap(cosine_sim, cmap=\"viridis\", annot=True, fmt=\".2f\")\n",
    "plt.title(\"Cosine Similarity with Temporal Masking\")\n",
    "plt.xlabel(\"Query\")\n",
    "plt.ylabel(\"Sequence Position\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_embedding.detach().numpy().shape, framewise_features.detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_wise_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_fewshot = pd.read_pickle(\n",
    "    \"/home/dl18206/Desktop/phd/code/personal/facebook/slowfast/dataset/results/r50_e100_fg_few_shot.pkl\"\n",
    ")\n",
    "bg_fewshot = pd.read_pickle(\n",
    "    \"/home/dl18206/Desktop/phd/code/personal/facebook/slowfast/dataset/results/r50_e100_bg_few_shot.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_sample = fg_fewshot.feat_map.iloc[0]\n",
    "bg_sample = bg_fewshot.feat_map.iloc[0]\n",
    "\n",
    "# Compute cosine similarity between the two samples\n",
    "fg_framewise_features = extract_frame_wise_features(fg_sample, t=16)\n",
    "bg_framewise_features = extract_frame_wise_features(bg_sample, t=16)\n",
    "\n",
    "# Squeeze and permute the dimensions\n",
    "fg_framewise_features = fg_framewise_features.squeeze(0)\n",
    "fg_framewise_features = fg_framewise_features.T\n",
    "\n",
    "bg_framewise_features = bg_framewise_features.squeeze(0)\n",
    "bg_framewise_features = bg_framewise_features.T\n",
    "\n",
    "# Compute framewise logits\n",
    "fg_frame_wise_logits = []\n",
    "for feat in fg_framewise_features.squeeze(0):\n",
    "    fg_frame_wise_logits.append(torch.sigmoid(classifier(feat).detach()).numpy())\n",
    "fg_frame_wise_logits = np.array(fg_frame_wise_logits)\n",
    "\n",
    "bg_frame_wise_logits = []\n",
    "for feat in bg_framewise_features.squeeze(0):\n",
    "    bg_frame_wise_logits.append(torch.sigmoid(classifier(feat).detach()).numpy())\n",
    "bg_frame_wise_logits = np.array(bg_frame_wise_logits)\n",
    "\n",
    "# Video feats\n",
    "fg_video_level_features = F.adaptive_avg_pool3d(fg_sample, (1, 1, 1))\n",
    "fg_video_level_features = torch.flatten(fg_video_level_features, start_dim=1)\n",
    "\n",
    "fg_video_level_logits = (\n",
    "    torch.sigmoid(classifier(fg_video_level_features)).detach().numpy()\n",
    ")\n",
    "fg_video_level_logits = np.array(fg_video_level_logits)\n",
    "\n",
    "bg_video_level_features = F.adaptive_avg_pool3d(bg_sample, (1, 1, 1))\n",
    "bg_video_level_features = torch.flatten(bg_video_level_features, start_dim=1)\n",
    "\n",
    "bg_video_level_logits = (\n",
    "    torch.sigmoid(classifier(bg_video_level_features)).detach().numpy()\n",
    ")\n",
    "bg_video_level_logits = np.array(bg_video_level_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fg and bg frame-wise logits\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10), gridspec_kw={\"width_ratios\": [1, 1]})\n",
    "sns.heatmap(\n",
    "    fg_frame_wise_logits.T, cmap=\"viridis\", annot=True, yticklabels=behaviours, ax=ax[0]\n",
    ")\n",
    "sns.heatmap(\n",
    "    bg_frame_wise_logits.T, cmap=\"viridis\", annot=True, yticklabels=behaviours, ax=ax[1]\n",
    ")\n",
    "\n",
    "ax[0].title.set_text(f\"FG Frame-wise Logits: {fg_fewshot.name.iloc[0][0]}\")\n",
    "ax[1].title.set_text(f\"BG Frame-wise Logits: {bg_fewshot.name.iloc[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fg and bg video-level logits\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 10), gridspec_kw={\"width_ratios\": [1, 1]})\n",
    "\n",
    "sns.heatmap(\n",
    "    fg_video_level_logits.T,\n",
    "    cmap=\"viridis\",\n",
    "    annot=True,\n",
    "    ax=ax[0],\n",
    "    cbar=False,\n",
    "    yticklabels=behaviours,\n",
    ")\n",
    "\n",
    "sns.heatmap(\n",
    "    bg_video_level_logits.T,\n",
    "    cmap=\"viridis\",\n",
    "    annot=True,\n",
    "    ax=ax[1],\n",
    "    cbar=False,\n",
    ")\n",
    "\n",
    "ax[0].title.set_text(f\"FG Video-level Logits: {fg_fewshot.name.iloc[0][0]}\")\n",
    "ax[1].title.set_text(f\"BG Video-level Logits: {bg_fewshot.name.iloc[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(fg_sample, bg_sample, feats=False, logits=False):\n",
    "    fg_framewise_features = extract_frame_wise_features(fg_sample, t=16)\n",
    "    bg_framewise_features = extract_frame_wise_features(bg_sample, t=16)\n",
    "\n",
    "    # Squeeze and permute the dimensions\n",
    "    fg_framewise_features = fg_framewise_features.squeeze(0)\n",
    "    fg_framewise_features = fg_framewise_features.T\n",
    "\n",
    "    bg_framewise_features = bg_framewise_features.squeeze(0)\n",
    "    bg_framewise_features = bg_framewise_features.T\n",
    "\n",
    "    # Compute framewise logits\n",
    "    fg_frame_wise_logits = []\n",
    "    for feat in fg_framewise_features.squeeze(0):\n",
    "        fg_frame_wise_logits.append(torch.sigmoid(classifier(feat).detach()).numpy())\n",
    "    fg_frame_wise_logits = np.array(fg_frame_wise_logits)\n",
    "\n",
    "    bg_frame_wise_logits = []\n",
    "    for feat in bg_framewise_features.squeeze(0):\n",
    "        bg_frame_wise_logits.append(torch.sigmoid(classifier(feat).detach()).numpy())\n",
    "    bg_frame_wise_logits = np.array(bg_frame_wise_logits)\n",
    "\n",
    "    # Video feats\n",
    "    fg_video_level_features = F.adaptive_avg_pool3d(fg_sample, (1, 1, 1))\n",
    "    fg_video_level_features = torch.flatten(fg_video_level_features, start_dim=1)\n",
    "\n",
    "    fg_video_level_logits = (\n",
    "        torch.sigmoid(classifier(fg_video_level_features)).detach().numpy()\n",
    "    )\n",
    "    fg_video_level_logits = np.array(fg_video_level_logits)\n",
    "\n",
    "    bg_video_level_features = F.adaptive_avg_pool3d(bg_sample, (1, 1, 1))\n",
    "    bg_video_level_features = torch.flatten(bg_video_level_features, start_dim=1)\n",
    "\n",
    "    bg_video_level_logits = (\n",
    "        torch.sigmoid(classifier(bg_video_level_features)).detach().numpy()\n",
    "    )\n",
    "    bg_video_level_logits = np.array(bg_video_level_logits)\n",
    "\n",
    "    if feats:\n",
    "        return fg_framewise_features, bg_framewise_features\n",
    "    elif logits:\n",
    "        return (\n",
    "            fg_frame_wise_logits,\n",
    "            bg_frame_wise_logits,\n",
    "            fg_video_level_logits,\n",
    "            bg_video_level_logits,\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            fg_framewise_features,\n",
    "            bg_framewise_features,\n",
    "            fg_frame_wise_logits,\n",
    "            bg_frame_wise_logits,\n",
    "            fg_video_level_logits,\n",
    "            bg_video_level_logits,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fg_sample, bg_sample, fg_name, bg_name, label in zip(\n",
    "    fg_fewshot.feat_map,\n",
    "    bg_fewshot.feat_map,\n",
    "    fg_fewshot.name,\n",
    "    bg_fewshot.name,\n",
    "    fs_val_df_neg.label,\n",
    "):\n",
    "    fg_framewise_features, bg_framewise_features = func(\n",
    "        fg_sample, bg_sample, feats=True\n",
    "    )\n",
    "\n",
    "    (\n",
    "        fg_frame_wise_logits,\n",
    "        bg_frame_wise_logits,\n",
    "        fg_video_level_logits,\n",
    "        bg_video_level_logits,\n",
    "    ) = func(fg_sample, bg_sample, logits=True)\n",
    "\n",
    "    # Plot fg and bg frame-wise logits\n",
    "    fig, ax = plt.subplots(\n",
    "        1, 5, figsize=(20, 10), gridspec_kw={\"width_ratios\": [1, 1, 0.25, 0.25, 0.25]}\n",
    "    )\n",
    "    sns.heatmap(\n",
    "        fg_frame_wise_logits.T,\n",
    "        cmap=\"viridis\",\n",
    "        annot=True,\n",
    "        yticklabels=behaviours,\n",
    "        ax=ax[0],\n",
    "    )\n",
    "    sns.heatmap(\n",
    "        bg_frame_wise_logits.T,\n",
    "        cmap=\"viridis\",\n",
    "        annot=True,\n",
    "        ax=ax[1],\n",
    "    )\n",
    "\n",
    "    sns.heatmap(\n",
    "        fg_video_level_logits.T,\n",
    "        cmap=\"viridis\",\n",
    "        annot=True,\n",
    "        ax=ax[2],\n",
    "        cbar=False,\n",
    "    )\n",
    "\n",
    "    sns.heatmap(\n",
    "        bg_video_level_logits.T,\n",
    "        cmap=\"viridis\",\n",
    "        annot=True,\n",
    "        ax=ax[3],\n",
    "        cbar=False,\n",
    "    )\n",
    "\n",
    "    sns.heatmap(\n",
    "        np.expand_dims(np.squeeze(np.array(ast.literal_eval(label))), axis=0).T,\n",
    "        cmap=\"viridis\",\n",
    "        annot=True,\n",
    "        ax=ax[4],\n",
    "        cbar=False,\n",
    "    )\n",
    "\n",
    "    ax[0].title.set_text(f\"FG Video-level Logits: {fg_name}\")\n",
    "    ax[1].title.set_text(f\"BG Video-level Logits: {bg_name}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create temporal mask using the background sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np array from (B,) to (1, B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slowfast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
