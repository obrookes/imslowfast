{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchmetrics.functional.classification import (\n",
    "    multilabel_average_precision,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['names', 'preds', 'feats', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/home/kukushkin/imslowfast/dataset/results/training_progression/validation/model=slow_r50_feats=epoch-1_split=val.pkl\"\n",
    "\n",
    "with open(file_path, \"rb\") as f:\n",
    "    data = pkl.load(f)\n",
    "\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load annotations and results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model name\n",
    "model_name = \"slow_r50\"\n",
    "folder_path = \"../dataset/results/training_progression\"\n",
    "splits = [\n",
    "    \"train\",\n",
    "    \"validation\",\n",
    "]\n",
    "metadata_file = \"../dataset/metadata/metadata.csv\"\n",
    "behavioural_labels_file = \"../dataset/metadata/behaviours.txt\"\n",
    "segements_file = \"../dataset/metadata/segements.txt\"\n",
    "\n",
    "# where to save the results\n",
    "result_file = \"../dataset/results/training_progression/results_training_progression_behaviourwise.csv\"\n",
    "show_per_class = True\n",
    "\n",
    "# list all result files in the folder which end with .pkl and contain the model name\n",
    "result_info = {}\n",
    "\n",
    "for split in splits:\n",
    "    for file in os.listdir(os.path.join(folder_path, split)):\n",
    "        if file.endswith(\".pkl\") and model_name in file:\n",
    "            epoch = file.split(\"_\")[-2].split(\"-\")[-1]\n",
    "\n",
    "            # get the split from the file name\n",
    "            data_split = file.split(\"=\")[-1].split(\".\")[0]\n",
    "\n",
    "            # add model to the dictionary\n",
    "            if model_name not in result_info:\n",
    "                result_info[model_name] = {}\n",
    "            # add epoch to the dictionary\n",
    "            if epoch not in result_info[model_name]:\n",
    "                result_info[model_name][epoch] = {}\n",
    "            if split not in result_info[model_name][epoch]:\n",
    "                result_info[model_name][epoch][data_split] = {}\n",
    "            result_info[model_name][epoch][data_split] = {\n",
    "                \"file_path\": os.path.join(folder_path, split, file),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'slow_r50': {'40': {'train': {'file_path': '../dataset/results/training_progression/train/model=slow_r50_feats=epoch-40_split=train.pkl'},\n",
       "   'val': {'file_path': '../dataset/results/training_progression/validation/model=slow_r50_feats=epoch-40_split=val.pkl'}},\n",
       "  '80': {'train': {'file_path': '../dataset/results/training_progression/train/model=slow_r50_feats=epoch-80_split=train.pkl'},\n",
       "   'val': {'file_path': '../dataset/results/training_progression/validation/model=slow_r50_feats=epoch-80_split=val.pkl'}},\n",
       "  '1': {'train': {'file_path': '../dataset/results/training_progression/train/model=slow_r50_feats=epoch-1_split=train.pkl'},\n",
       "   'val': {'file_path': '../dataset/results/training_progression/validation/model=slow_r50_feats=epoch-1_split=val.pkl'}},\n",
       "  '10': {'train': {'file_path': '../dataset/results/training_progression/train/model=slow_r50_feats=epoch-10_split=train.pkl'},\n",
       "   'val': {'file_path': '../dataset/results/training_progression/validation/model=slow_r50_feats=epoch-10_split=val.pkl'}},\n",
       "  '160': {'train': {'file_path': '../dataset/results/training_progression/train/model=slow_r50_feats=epoch-160_split=train.pkl'},\n",
       "   'val': {'file_path': '../dataset/results/training_progression/validation/model=slow_r50_feats=epoch-160_split=val.pkl'}},\n",
       "  '20': {'train': {'file_path': '../dataset/results/training_progression/train/model=slow_r50_feats=epoch-20_split=train.pkl'},\n",
       "   'val': {'file_path': '../dataset/results/training_progression/validation/model=slow_r50_feats=epoch-20_split=val.pkl'}},\n",
       "  '5': {'train': {'file_path': '../dataset/results/training_progression/train/model=slow_r50_feats=epoch-5_split=train.pkl'},\n",
       "   'val': {'file_path': '../dataset/results/training_progression/validation/model=slow_r50_feats=epoch-5_split=val.pkl'}}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_csv(metadata_file)\n",
    "\n",
    "with open(behavioural_labels_file, \"rb\") as f:\n",
    "    behaviours = [beh.decode(\"utf-8\").strip() for beh in f.readlines()]\n",
    "\n",
    "with open(segements_file, \"rb\") as f:\n",
    "    segments = [seg.decode(\"utf-8\").strip() for seg in f.readlines()]\n",
    "\n",
    "\n",
    "def read_files(model_results, epoch):\n",
    "    with open(model_results[epoch][\"train\"][\"file_path\"], \"rb\") as f:\n",
    "        train_data = pkl.load(f)\n",
    "\n",
    "    with open(model_results[epoch][\"val\"][\"file_path\"], \"rb\") as f:\n",
    "        val_data = pkl.load(f)\n",
    "\n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "def results2df(train_data, val_data, metadata_df):\n",
    "    # Process subclips\n",
    "    subclips = []\n",
    "    for i, split in enumerate([train_data, val_data]):\n",
    "        for name, pred, feat, label in zip(\n",
    "            split[\"names\"], split[\"preds\"], split[\"feats\"], split[\"labels\"]\n",
    "        ):\n",
    "            subclips.append(\n",
    "                {\n",
    "                    \"name\": name,\n",
    "                    \"split\": i,\n",
    "                    \"pred\": pred,\n",
    "                    \"feat\": feat,\n",
    "                    \"negative\": True if sum(label) == 0 else False,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df = pd.DataFrame(subclips, columns=[\"name\", \"split\", \"pred\", \"feat\", \"negative\"])\n",
    "\n",
    "    df[\"split\"] = df.split.map({0: \"train\", 1: \"val\"})\n",
    "    df = df.merge(metadata_df, how=\"left\", left_on=\"name\", right_on=\"subject_id\")\n",
    "\n",
    "    # Apply sigmoid to predictions\n",
    "    df[\"pred\"] = df.pred.apply(lambda x: torch.sigmoid(torch.tensor(x)))\n",
    "\n",
    "    # Convert label from str to int\n",
    "    df.label = df.label.apply(lambda x: np.array(ast.literal_eval(x)))\n",
    "\n",
    "    # Add negative\n",
    "    df[\"negative\"] = df.label.apply(lambda x: sum(x) == 0)\n",
    "\n",
    "    # Add global location count to dataframe\n",
    "    df[\"location_count\"] = df.utm.map(df.utm.value_counts())\n",
    "\n",
    "    # Return train and val dataframes\n",
    "    train_df = df[df.split == \"train\"]\n",
    "    val_df = df[df.split == \"val\"]\n",
    "\n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "def print_per_segement_performance(map, segment, show_per_class=True):\n",
    "    res = []\n",
    "    for i, (b, s) in enumerate(zip(map, segments)):\n",
    "        if s == segment:\n",
    "            res.append({behaviours[i]: b})\n",
    "    agg_values = []\n",
    "    for r in res:\n",
    "        for _, value in r.items():\n",
    "            agg_values.append(value)\n",
    "    # if show_per_class:\n",
    "    #    print(f\"{segment}: {np.mean(agg_values):.2f} {res}\")\n",
    "    # else:\n",
    "    #    print(f\"{segment}: {np.mean(agg_values):.2f}\")\n",
    "\n",
    "    if show_per_class:\n",
    "        return {\n",
    "            segment: {\n",
    "                \"mean\": np.round(np.mean(agg_values), 2),\n",
    "                \"values\": res,\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            segment: {\n",
    "                \"mean\": np.round(np.mean(agg_values), 2),\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df, round_to=2, show_per_class=False):\n",
    "    # Train performance\n",
    "    map = multilabel_average_precision(\n",
    "        torch.tensor(np.stack(df[\"pred\"])),\n",
    "        torch.tensor(np.stack(df[\"label\"])),\n",
    "        num_labels=14,\n",
    "        average=\"none\",\n",
    "    )\n",
    "\n",
    "    map_head = print_per_segement_performance(map, \"head\", show_per_class)\n",
    "    map_tail = print_per_segement_performance(map, \"tail\", show_per_class)\n",
    "    map_fs = print_per_segement_performance(map, \"few_shot\", show_per_class)\n",
    "\n",
    "    if show_per_class:\n",
    "        map_head_values = map_head[\"head\"][\"values\"]\n",
    "        map_tail_values = map_tail[\"tail\"][\"values\"]\n",
    "        map_fs_values = map_fs[\"few_shot\"][\"values\"]\n",
    "\n",
    "    map_head = round(float(map_head[\"head\"][\"mean\"]), round_to)\n",
    "    map_tail = round(float(map_tail[\"tail\"][\"mean\"]), round_to)\n",
    "    map_fs = round(float(map_fs[\"few_shot\"][\"mean\"]), round_to)\n",
    "\n",
    "    avg_map = round(map.mean().item(), round_to)\n",
    "\n",
    "    if show_per_class:\n",
    "        # round to 2 decimal places\n",
    "        print(map_head_values)\n",
    "        for i in range(len(map_head_values)):\n",
    "            for key, value in map_head_values[i].items():\n",
    "                map_head_values[i][key] = round(value.item(), round_to)\n",
    "        for i in range(len(map_tail_values)):\n",
    "            for key, value in map_tail_values[i].items():\n",
    "                map_tail_values[i][key] = round(value.item(), round_to)\n",
    "\n",
    "        for i in range(len(map_fs_values)):\n",
    "            for key, value in map_fs_values[i].items():\n",
    "                map_fs_values[i][key] = round(value.item(), round_to)\n",
    "\n",
    "        return (\n",
    "            avg_map,\n",
    "            map_head,\n",
    "            map_head_values,\n",
    "            map_tail,\n",
    "            map_tail_values,\n",
    "            map_fs,\n",
    "            map_fs_values,\n",
    "        )\n",
    "\n",
    "    return avg_map, map_head, map_tail, map_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'resting': tensor(0.9601)}, {'travel': tensor(0.9739)}]\n",
      "[{'resting': tensor(0.6850)}, {'travel': tensor(0.8118)}]\n",
      "[{'resting': tensor(0.9942)}, {'travel': tensor(0.9899)}]\n",
      "[{'resting': tensor(0.6567)}, {'travel': tensor(0.7795)}]\n",
      "[{'resting': tensor(0.4930)}, {'travel': tensor(0.3583)}]\n",
      "[{'resting': tensor(0.4871)}, {'travel': tensor(0.3139)}]\n",
      "[{'resting': tensor(0.7470)}, {'travel': tensor(0.9032)}]\n",
      "[{'resting': tensor(0.5894)}, {'travel': tensor(0.8289)}]\n",
      "[{'resting': tensor(0.9994)}, {'travel': tensor(0.9986)}]\n",
      "[{'resting': tensor(0.6571)}, {'travel': tensor(0.7116)}]\n",
      "[{'resting': tensor(0.8854)}, {'travel': tensor(0.9520)}]\n",
      "[{'resting': tensor(0.6755)}, {'travel': tensor(0.8202)}]\n",
      "[{'resting': tensor(0.6577)}, {'travel': tensor(0.6468)}]\n",
      "[{'resting': tensor(0.5887)}, {'travel': tensor(0.4751)}]\n"
     ]
    }
   ],
   "source": [
    "# if file exists, delete it\n",
    "if os.path.exists(result_file):\n",
    "    os.remove(result_file)\n",
    "\n",
    "\n",
    "for m in result_info:\n",
    "    for epoch in result_info[m]:\n",
    "        # print(f\"Loading results for model: {m}, epoch: {epoch}\")\n",
    "        train_data, val_data = read_files(result_info[model_name], epoch)\n",
    "        train_df, val_df = results2df(train_data, val_data, metadata_df)\n",
    "\n",
    "        if show_per_class:\n",
    "            (\n",
    "                train_map,\n",
    "                train_map_head,\n",
    "                train_map_head_values,\n",
    "                train_map_tail,\n",
    "                train_map_tail_values,\n",
    "                train_map_fs,\n",
    "                train_map_fs_values,\n",
    "            ) = calculate_metrics(train_df, show_per_class=show_per_class)\n",
    "\n",
    "            (\n",
    "                val_map,\n",
    "                val_map_head,\n",
    "                val_map_head_values,\n",
    "                val_map_tail,\n",
    "                val_map_tail_values,\n",
    "                val_map_fs,\n",
    "                val_map_fs_values,\n",
    "            ) = calculate_metrics(val_df, show_per_class=show_per_class)\n",
    "\n",
    "            with open(result_file, \"a\") as f:\n",
    "                # check if file is existing and empty\n",
    "                if os.stat(result_file).st_size == 0:\n",
    "                    f.write(\n",
    "                        \"model;split;epoch;overall_map;map_head;map_head_values;map_tail;map_tail_values;map_fs;map_fs_values\\n\"\n",
    "                    )\n",
    "\n",
    "                f.write(\n",
    "                    f\"{m};train;{epoch};{train_map};{train_map_head};{train_map_head_values};{train_map_tail};{train_map_tail_values};{train_map_fs};{train_map_fs_values}\\n\"\n",
    "                )\n",
    "                f.write(\n",
    "                    f\"{m};val;{epoch};{val_map};{val_map_head};{val_map_head_values};{val_map_tail};{val_map_tail_values};{val_map_fs};{val_map_fs_values}\\n\"\n",
    "                )\n",
    "\n",
    "            # Write results to file # with columns: model, epoch, train_map, train_map_head, train_map_tail, train_map_fs, val_map, val_map_head, val_map_tail, val_map_fs\n",
    "        else:\n",
    "            with open(result_file, \"a\") as f:\n",
    "                # check if file is existing and empty\n",
    "                if os.stat(result_file).st_size == 0:\n",
    "                    f.write(\"model;split;epoch;overall_map;map_head;map_tail;map_fs\\n\")\n",
    "\n",
    "                f.write(\n",
    "                    f\"{m};train;{epoch};{train_map};{train_map_head};{train_map_tail};{train_map_fs}\\n\"\n",
    "                )\n",
    "                f.write(\n",
    "                    f\"{m};val;{epoch};{val_map};{val_map_head};{val_map_tail};{val_map_fs}\\n\"\n",
    "                )\n",
    "\n",
    "\n",
    "# open the csv reorder the columns and save it again\n",
    "df = pd.read_csv(result_file, sep=\";\")\n",
    "\n",
    "if show_per_class:\n",
    "    df = df[\n",
    "        [\n",
    "            \"model\",\n",
    "            \"split\",\n",
    "            \"epoch\",\n",
    "            \"overall_map\",\n",
    "            \"map_head\",\n",
    "            \"map_head_values\",\n",
    "            \"map_tail\",\n",
    "            \"map_tail_values\",\n",
    "            \"map_fs\",\n",
    "            \"map_fs_values\",\n",
    "        ]\n",
    "    ]\n",
    "else:\n",
    "    df = df[\n",
    "        [\n",
    "            \"model\",\n",
    "            \"split\",\n",
    "            \"epoch\",\n",
    "            \"overall_map\",\n",
    "            \"map_head\",\n",
    "            \"map_tail\",\n",
    "            \"map_fs\",\n",
    "        ]\n",
    "    ]\n",
    "df.epoch = df.epoch.astype(int)\n",
    "df = df.sort_values(\n",
    "    [\n",
    "        \"model\",\n",
    "        \"epoch\",\n",
    "        \"split\",\n",
    "    ],\n",
    "    ascending=[True, True, True],\n",
    ")\n",
    "df.to_csv(result_file, index=False, sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataset-upgrade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
