{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from adjustText import adjust_text\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.gridspec import GridSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"../model=slow-50-tap/model=slow-50_method=tap-16x8-w-negatives_ext=sw-0.5_return-feats=train_feats.pkl\",\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    data = pkl.load(f)\n",
    "\n",
    "metadata = pd.read_csv(\n",
    "    \"/home/dl18206/Desktop/phd/data/panaf/PanAfFull/notebooks/data/panaf-seq_metadata.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"../model=slow-50-tap/model=slow_r50-w-negatives_type=return_train_feats_feats.pkl\",\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    g_data = pkl.load(f)\n",
    "\n",
    "with open(\"../data/behaviours.txt\", \"rb\") as f:\n",
    "    behaviours = [beh.decode(\"utf-8\").strip() for beh in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subclips = []\n",
    "for name, feat, cls_act_seq, global_feat, global_pred in zip(\n",
    "    data[\"names\"], data[\"feats\"], data[\"cas\"], g_data[\"feats\"], g_data[\"preds\"]\n",
    "):\n",
    "    recon_mean = []\n",
    "    for i, (f, c) in enumerate(zip(feat, cls_act_seq)):\n",
    "        subclips.append(\n",
    "            {\n",
    "                \"name\": name,\n",
    "                \"feat\": f.detach().cpu().numpy(),\n",
    "                \"cas\": c.detach().cpu().numpy(),\n",
    "                \"timestep\": i,\n",
    "                \"point_type\": \"local\",\n",
    "            }\n",
    "        )\n",
    "        if i == 0:\n",
    "            recon_mean.append(f.detach().cpu().numpy())\n",
    "        else:\n",
    "            recon_mean.append(f.detach().cpu().numpy())\n",
    "\n",
    "    subclips.append(\n",
    "        {\n",
    "            \"name\": name,\n",
    "            \"feat\": global_feat.detach().cpu().numpy(),\n",
    "            \"cas\": global_pred.detach().cpu().numpy(),\n",
    "            \"timestep\": i + 1,\n",
    "            \"point_type\": \"global\",\n",
    "        }\n",
    "    )\n",
    "    subclips.append(\n",
    "        {\n",
    "            \"name\": name,\n",
    "            \"feat\": np.mean(recon_mean, axis=0),\n",
    "            \"cas\": global_pred.detach().cpu().numpy(),\n",
    "            \"timestep\": i + 2,\n",
    "            \"point_type\": \"reconstructed\",\n",
    "        }\n",
    "    )\n",
    "df = pd.DataFrame(subclips, columns=[\"name\", \"feat\", \"cas\", \"timestep\", \"point_type\"])\n",
    "df = df.merge(metadata, how=\"left\", left_on=\"name\", right_on=\"subject_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"softmax\"] = df.cas.apply(lambda x: torch.softmax(torch.tensor(x), dim=0))\n",
    "df[\"topk_cls_idx\"] = df.softmax.apply(lambda x: torch.topk(x, 3).indices.numpy())\n",
    "df[\"topk_cls_val\"] = df.softmax.apply(lambda x: torch.topk(x, 3).values.numpy())\n",
    "\n",
    "# Round the cls values to 2 decimal places\n",
    "df[\"topk_cls_val\"] = df[\"topk_cls_val\"].apply(lambda x: [round(i, 2) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE on the features and add x1 and x2 columns to the dataframe\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "\n",
    "tsne_results = tsne.fit_transform(df[\"feat\"].tolist())\n",
    "df[\"x1\"] = tsne_results[:, 0]\n",
    "df[\"x2\"] = tsne_results[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_behaviours(x, tgt_b):\n",
    "    gt_seq = x.split(\",\")\n",
    "    for i in gt_seq:\n",
    "        if i in tgt_b:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def plot_tsne_results(\n",
    "    df,\n",
    "    colour_by=[\"name\"],\n",
    "    annotate_by=\"timestep\",\n",
    "    batch_size=320,\n",
    "    fig_size=(5, 3),\n",
    "    save_name=False,\n",
    "    behaviours=None,\n",
    "    exact_match=False,\n",
    "    videos=None,\n",
    "    show_global=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot t-SNE results for subclip embeddings.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing the subclip embeddings.\n",
    "        colour_by (str or list, optional): The column(s) to use for coloring the scatter plot(s). Defaults to [\"name\"].\n",
    "        annotate_by (str, optional): The column to use for annotating the scatter plot. Defaults to \"timestep\".\n",
    "        batch_size (int, optional): The number of data points to include in the scatter plot. Defaults to 320.\n",
    "        fig_size (tuple, optional): The size of the figure. Defaults to (10, 6).\n",
    "        save_name (bool or str, optional): If True, the plot will be saved with the specified name.\n",
    "            If False, the plot will not be saved. If a string is provided, the plot will be saved with the specified name.\n",
    "            Defaults to False.\n",
    "        behaviours (list, optional): The list of behaviors to filter the data by. Defaults to None.\n",
    "        exact_match (bool, optional): If True, only subclip embeddings with behaviors exactly matching the provided list will be included in the plot.\n",
    "            If False, subclip embeddings with behaviors containing any of the provided list will be included in the plot.\n",
    "            Defaults to False.\n",
    "        videos (list, optional): The list of videos to filter the data by. Defaults to None.\n",
    "        name (str, optional): The name variable for the plot. Defaults to None.\n",
    "        show_global (bool, optional): If True, global subclip embeddings will be included in the plot.\n",
    "            If False, only local subclip embeddings will be included in the plot. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Convert colour_by to list if it's a string\n",
    "    if isinstance(colour_by, str):\n",
    "        colour_by = [colour_by]\n",
    "\n",
    "    # Calculate the number of rows and columns for subplots\n",
    "    n_plots = len(colour_by)\n",
    "    n_cols = min(3, n_plots)  # Maximum 3 columns\n",
    "    n_rows = (n_plots - 1) // n_cols + 1\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(\n",
    "        n_rows, n_cols, figsize=(fig_size[0] * n_cols, fig_size[1] * n_rows)\n",
    "    )\n",
    "    if n_plots == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    # Filter by behaviours(s)\n",
    "    if behaviours is not None:\n",
    "        if exact_match:\n",
    "            df = df[df.value.apply(lambda x: set(behaviours) == set(x.split(\",\")))]\n",
    "        else:\n",
    "            df = df[df.value.apply(lambda x: filter_behaviours(x, behaviours))]\n",
    "\n",
    "    # Filter by video(s)\n",
    "    if videos is not None:\n",
    "        df = df[df.name.isin(videos)]\n",
    "\n",
    "    # Remove plot_type if == \"global\"\n",
    "    if not show_global:\n",
    "        df = df[df.point_type == \"local\"]\n",
    "\n",
    "    # Create scatter plots for each colour_by variable\n",
    "    for i, col in enumerate(colour_by):\n",
    "        ax = axes[i]\n",
    "        scatter = sns.scatterplot(\n",
    "            x=\"x1\",\n",
    "            y=\"x2\",\n",
    "            hue=col,\n",
    "            data=df.head(batch_size),\n",
    "            legend=\"full\",\n",
    "            alpha=1.0,\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "        # Prepare annotations\n",
    "        if annotate_by is not None:\n",
    "            texts = []\n",
    "            for idx, row in df.head(batch_size).iterrows():\n",
    "                texts.append(\n",
    "                    ax.text(\n",
    "                        row[\"x1\"],\n",
    "                        row[\"x2\"],\n",
    "                        str(row[annotate_by].values[0]),\n",
    "                        fontsize=8,\n",
    "                        alpha=0.7,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # Adjust text positions to prevent overlap\n",
    "            adjust_text(\n",
    "                texts,\n",
    "                arrowprops=dict(arrowstyle=\"->\", color=\"red\", lw=0.5),\n",
    "                expand_points=(1.2, 1.2),\n",
    "                force_points=(0.1, 0.25),\n",
    "                ax=ax,\n",
    "            )\n",
    "\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0, ncol=1)\n",
    "\n",
    "        if behaviours is not None:\n",
    "            behaviours = (\n",
    "                behaviours[0] if len(behaviours) == 1 else \", \".join(behaviours)\n",
    "            )\n",
    "        else:\n",
    "            behaviours = \"random\"\n",
    "\n",
    "        ax.set_title(\n",
    "            f\"Subclips: 16, Frames: 8, Behaviours: {behaviours}, Colored by: {col}\"\n",
    "        )\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for i in range(n_plots, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_name:\n",
    "        plt.savefig(f\"{save_name}.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def return_video_names(df, classes, exact_match, batch_size):\n",
    "#     # Filter by behaviours(s)\n",
    "#     if behaviours is not None:\n",
    "#         if exact_match:\n",
    "#             df = df[df.value.apply(lambda x: set(classes) == set(x.split(\",\")))]\n",
    "#         else:\n",
    "#             df = df[df.value.apply(lambda x: filter_behaviours(x, classes))]\n",
    "#     return df.head(batch_size)[\"name\"].unique()\n",
    "\n",
    "\n",
    "# classes = [\"display\", \"aggression\", \"piloerection\"]\n",
    "\n",
    "# video_names_iter = return_video_names(df, classes, False, 640)\n",
    "# video_names_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-71-a765feaed23d>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  global_df['label'] = global_df['label'].apply(lambda x: ast.literal_eval(x))\n"
     ]
    }
   ],
   "source": [
    "global_df = df[df.point_type == \"global\"]\n",
    "global_df[\"label\"] = global_df[\"label\"].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "cls_feat = []\n",
    "sample_feat = []\n",
    "\n",
    "for idx in range(len(behaviours)):\n",
    "    features = []\n",
    "    purities = []\n",
    "    tmp_df = global_df[global_df.label.apply(lambda x: x[idx] == 1)]\n",
    "    for row in tmp_df.iterrows():\n",
    "        feature = row[1][\"feat\"]\n",
    "        purity = 1 / sum(row[1][\"label\"])\n",
    "        features.append(feature)\n",
    "        purities.append(purity)\n",
    "    cls_feat.append(np.average(features, axis=0, weights=purities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_df.label.apply(lambda x: x[idx] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 2048)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl18206/anaconda3/envs/dataset-upgrade/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py:795: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "/home/dl18206/anaconda3/envs/dataset-upgrade/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py:805: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 13 nearest neighbors...\n",
      "[t-SNE] Indexed 14 samples in 0.001s...\n",
      "[t-SNE] Computed neighbors for 14 samples in 0.307s...\n",
      "[t-SNE] Computed conditional probabilities for sample 14 / 14\n",
      "[t-SNE] Mean sigma: 1125899906842624.000000\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 101.664566\n",
      "[t-SNE] KL divergence after 300 iterations: 0.551155\n"
     ]
    }
   ],
   "source": [
    "# Perform t-SNE on the features and add x1 and x2 columns to the dataframe\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(np.stack(cls_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the t-SNE results\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=\"r\")\n",
    "plt.title(\"Global Features t-SNE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_results(\n",
    "    df,\n",
    "    colour_by=[\"name\", \"point_type\"],\n",
    "    annotate_by=None,  # [\"topk_cls_idx\"],\n",
    "    batch_size=1000,\n",
    "    fig_size=(10, 6),\n",
    "    # behaviours=classes,\n",
    "    exact_match=False,\n",
    "    show_global=True,\n",
    "    videos=[\"acp000bhwx.mp4\", \"acp0005a6i.mp4\", \"acp0005a9y.mp4\", \"acp000dkt6.mp4\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_results(\n",
    "    df,\n",
    "    colour_by=[\"name\"],\n",
    "    annotate_by=None,  # [\"topk_cls_idx\"],\n",
    "    batch_size=48,\n",
    "    fig_size=(10, 6),\n",
    "    behaviours=classes,\n",
    "    exact_match=False,\n",
    "    show_global=False,\n",
    "    videos=[\"acp000b8ft.mp4\", \"acp000c7n9.mp4\", \"acp0005a6g.mp4\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'behaviours' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-49b194610a6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbehaviours\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'behaviours' is not defined"
     ]
    }
   ],
   "source": [
    "dict(enumerate(behaviours))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_clips(\n",
    "    video_tensor, n_clips, n_samples, overlap=0.25, sliding_window=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Converts a video tensor into a list of clips.\n",
    "\n",
    "    Args:\n",
    "        video_tensor (torch.Tensor): The input video tensor of shape (T, C, H, W), where T is the number of frames,\n",
    "                                     C is the number of channels, H is the height, and W is the width.\n",
    "        n_clips (int): The desired number of clips to generate.\n",
    "        n_samples (int): The number of samples to subsample from each clip.\n",
    "        overlap (float, optional): The overlap ratio between adjacent clips. Defaults to 0.25.\n",
    "        sliding_window (bool, optional): If True, uses a sliding window approach to generate clips. If False,\n",
    "                                         divides the video tensor into equal-sized clips.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of clips, where each clip is a tensor of shape (T_clip, C, H, W), and T_clip is the number of frames\n",
    "              in each clip.\n",
    "    \"\"\"\n",
    "    clips = []\n",
    "    T, C, H, W = video_tensor.shape\n",
    "    if sliding_window:\n",
    "        # Calculate the size of each clip\n",
    "        clip_size = math.ceil(T / (n_clips * (1 - overlap) + overlap))\n",
    "\n",
    "        # Calculate stride based on the overlap\n",
    "        stride = math.floor(clip_size * (1 - overlap))\n",
    "\n",
    "        start = 0\n",
    "        while start + clip_size <= T:\n",
    "            clip = video_tensor[start : start + clip_size]\n",
    "            ss_clip = subsample_clip(clip, n_samples)\n",
    "            clips.append(ss_clip)\n",
    "            start += stride\n",
    "\n",
    "        # If we haven't generated enough clips, add the last clip\n",
    "        if len(clips) < n_clips:\n",
    "            start = T - clip_size\n",
    "            clip = video_tensor[start:T]\n",
    "            ss_clip = subsample_clip(clip, n_samples)\n",
    "            clips.append(ss_clip)\n",
    "\n",
    "        # If we have generated too many clips, keep the first n_clips\n",
    "        clips = clips[:n_clips]\n",
    "    else:\n",
    "        T_per_clip = T // n_clips\n",
    "        for i in range(n_clips):\n",
    "            ss_clip = subsample_clip(\n",
    "                video_tensor[i * T_per_clip : (i + 1) * T_per_clip], n_samples\n",
    "            )\n",
    "            clips.append(ss_clip)\n",
    "    return clips\n",
    "\n",
    "\n",
    "def subsample_clip(clip, n_samples):\n",
    "    T, C, H, W = clip.shape\n",
    "    if T <= n_samples:\n",
    "        return clip\n",
    "\n",
    "    indices = torch.linspace(0, T - 1, n_samples).long()\n",
    "    return clip[indices]\n",
    "\n",
    "\n",
    "def visualize_logits_and_labels(\n",
    "    logits, global_logits, labels, behaviours, class_seq, video_name\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize subclip logits, global logits, and multi-hot encoded labels side by side.\n",
    "\n",
    "    Parameters:\n",
    "    logits (np.array): Array of shape [n_clips, num_classes] containing logits\n",
    "    global_logits (np.array): Array of shape [1, num_classes] containing global logits\n",
    "    labels (np.array): Array of shape [1, num_classes] containing multi-hot encoded labels\n",
    "    behaviours (list): List of behaviour labels for y-axis\n",
    "    class_seq (str): Title for the logits subplot\n",
    "\n",
    "    Returns:\n",
    "    fig (matplotlib.figure.Figure): The created figure\n",
    "    \"\"\"\n",
    "    # Apply sigmoid function to logits and global logits\n",
    "    sigmoid_logits = 1 / (1 + np.exp(-logits))\n",
    "    sigmoid_global_logits = 1 / (1 + np.exp(-global_logits))\n",
    "\n",
    "    # Create a custom colormap that goes from white to blue\n",
    "    colors = [(1, 1, 1), (0, 0, 1)]  # White to Blue\n",
    "    n_bins = 100  # Number of color gradations\n",
    "    cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors, N=n_bins)\n",
    "\n",
    "    # Create a figure with a custom grid\n",
    "    fig = plt.figure(figsize=(24, 8))\n",
    "    gs = GridSpec(1, 4, figure=fig, width_ratios=[8, 1, 1, 0.2])\n",
    "\n",
    "    # Subfigure 1: Sigmoid-activated logits\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    im1 = ax1.imshow(sigmoid_logits.T, aspect=\"auto\", cmap=cmap, vmin=0, vmax=1)\n",
    "    ax1.set_xlabel(\"Subclips\")\n",
    "    ax1.set_ylabel(\"Classes\")\n",
    "    ax1.set_title(f\"{video_name}: {class_seq}\")\n",
    "\n",
    "    # Add text annotations for sigmoid logits\n",
    "    for i in range(sigmoid_logits.shape[1]):\n",
    "        for j in range(sigmoid_logits.shape[0]):\n",
    "            ax1.text(\n",
    "                j,\n",
    "                i,\n",
    "                f\"{sigmoid_logits[j, i]:.2f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                color=\"black\",\n",
    "                fontsize=10,\n",
    "            )\n",
    "\n",
    "    # Subfigure 2: Global logits\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    im2 = ax2.imshow(sigmoid_global_logits.T, aspect=\"auto\", cmap=cmap, vmin=0, vmax=1)\n",
    "    ax2.set_title(\"Global Logits\")\n",
    "\n",
    "    # Add text annotations for global logits\n",
    "    for i in range(sigmoid_global_logits.shape[1]):\n",
    "        ax2.text(\n",
    "            0,\n",
    "            i,\n",
    "            f\"{sigmoid_global_logits[0, i]:.2f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=\"black\",\n",
    "            fontsize=10,\n",
    "        )\n",
    "\n",
    "    # Subfigure 3: Multi-hot encoded labels\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    im3 = ax3.imshow(labels.T, aspect=\"auto\", cmap=cmap, vmin=0, vmax=1)\n",
    "    ax3.set_title(\"Multi-hot Encoded Label\")\n",
    "\n",
    "    # Add text annotations for labels\n",
    "    for i in range(labels.shape[1]):\n",
    "        ax3.text(\n",
    "            0,\n",
    "            i,\n",
    "            str(int(labels[0, i])),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=\"black\",\n",
    "            fontsize=10,\n",
    "        )\n",
    "\n",
    "    # Set ticks for subfigures\n",
    "    ax1.set_xticks(np.arange(0, sigmoid_logits.shape[0]))\n",
    "    ax1.set_xticklabels(np.arange(0, sigmoid_logits.shape[0]))\n",
    "    ax1.set_yticks(np.arange(sigmoid_logits.shape[1]))\n",
    "    ax1.set_yticklabels(behaviours)\n",
    "\n",
    "    for ax in [ax2, ax3]:\n",
    "        ax.set_xticks([])  # Remove x-ticks\n",
    "        ax.set_xticklabels([])  # Remove x-tick labels\n",
    "        ax.set_yticks(np.arange(labels.shape[1]))\n",
    "        ax.set_yticklabels([])  # Remove y-tick labels\n",
    "\n",
    "    plt.setp(ax1.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # Add a single colorbar to the right of the labels subplot\n",
    "    cax = fig.add_subplot(gs[0, 3])\n",
    "    cbar = fig.colorbar(im1, cax=cax)\n",
    "    cbar.set_label(\"Activation Strength / Label Presence\")\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_video(video_name, n_clips, n_samples, sliding_window=False):\n",
    "\n",
    "    path2video = f\"/home/dl18206/Desktop/phd/data/panaf/acp/videos/all/{video_name}\"\n",
    "\n",
    "    video = torchvision.io.read_video(path2video, pts_unit=\"sec\")[0]\n",
    "\n",
    "    clips = video_to_clips(video, n_clips, n_samples, sliding_window=sliding_window)\n",
    "    clips = rearrange(torch.stack(clips), \"n t h w c -> (n t) c h w\")\n",
    "\n",
    "    grid = make_grid(clips, nrow=n_samples)\n",
    "    img = torchvision.transforms.ToPILImage()(grid)\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(df, vid):\n",
    "    class_act_seq = df[df.name == vid][\"cas\"].values\n",
    "    class_act_seq = np.vstack(class_act_seq)\n",
    "\n",
    "    global_logits = df[(df.name == vid) & (df.timestep == 16)][\"cas\"].values[0]\n",
    "    global_logits = global_logits.reshape(1, -1)\n",
    "\n",
    "    labels = df[df.name == vid][\"label\"].values[0]\n",
    "    labels = ast.literal_eval(labels)\n",
    "    labels = np.array(labels).reshape(1, -1)\n",
    "\n",
    "    class_seq = df[df.name == vid][\"value\"].values[0]\n",
    "    class_seq = class_seq.replace(\",\", \" -> \")\n",
    "\n",
    "    return class_act_seq[:-1,], global_logits, labels, class_seq\n",
    "\n",
    "\n",
    "def plot_tsne_results_subplot(\n",
    "    ax,\n",
    "    df,\n",
    "    colour_by,\n",
    "    annotate_by,\n",
    "    video_name,\n",
    "    batch_size,\n",
    "    only_behaviours=None,\n",
    "    exact_match=False,\n",
    "    show_global=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot t-SNE results on a given subplot.\n",
    "    \"\"\"\n",
    "    # Filter by behaviours(s)\n",
    "    if only_behaviours is not None:\n",
    "        if exact_match:\n",
    "            df = df[df.value.apply(lambda x: set(only_behaviours) == set(x.split(\",\")))]\n",
    "        else:\n",
    "            df = df[df.value.apply(lambda x: filter_behaviours(x, only_behaviours))]\n",
    "\n",
    "    # Filter by video_name\n",
    "    df = df[df.name.isin([video_name])]\n",
    "\n",
    "    if not show_global:\n",
    "        df = df[df.point_type == \"local\"]\n",
    "\n",
    "    scatter = sns.scatterplot(\n",
    "        x=\"x1\",\n",
    "        y=\"x2\",\n",
    "        hue=colour_by,\n",
    "        data=df.head(batch_size),\n",
    "        legend=\"full\",\n",
    "        alpha=1.0,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    if annotate_by is not None:\n",
    "        texts = []\n",
    "        for idx, row in df.head(batch_size).iterrows():\n",
    "            texts.append(\n",
    "                ax.text(\n",
    "                    row[\"x1\"], row[\"x2\"], str(row[annotate_by]), fontsize=6, alpha=0.7\n",
    "                )\n",
    "            )\n",
    "\n",
    "        adjust_text(\n",
    "            texts,\n",
    "            arrowprops=dict(arrowstyle=\"->\", color=\"red\", lw=0.5),\n",
    "            expand_points=(1.2, 1.2),\n",
    "            force_points=(0.1, 0.25),\n",
    "        )\n",
    "\n",
    "    ax.legend(\n",
    "        bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0, ncol=1, fontsize=\"x-small\"\n",
    "    )\n",
    "    ax.set_title(f\"t-SNE: Subclip embs coloured by {colour_by}\")\n",
    "\n",
    "\n",
    "def visualize_logits_labels_video_and_tsne(\n",
    "    df,\n",
    "    video_name,\n",
    "    behaviours,\n",
    "    n_clips,\n",
    "    n_samples,\n",
    "    exact_match=False,\n",
    "    save_prefix=None,\n",
    "    sliding_window=False,\n",
    "    colour_by=\"name\",\n",
    "    annotate_by=\"timestep\",\n",
    "    batch_size=160,\n",
    "    show_global=False,\n",
    "    video_frame_width=3,  # New parameter for video frame width\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize subclip logits, global logits, multi-hot encoded labels, video frames, and t-SNE plot side-by-side.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data for visualization\n",
    "    video_name (str): Name of the video\n",
    "    behaviours (list): List of behaviour labels for y-axis\n",
    "    n_clips (int): Number of clips\n",
    "    n_samples (int): Number of samples per clip\n",
    "    sliding_window (bool): Whether to use sliding window for clip generation\n",
    "    colour_by (str): Column to use for coloring the t-SNE plot\n",
    "    annotate_by (str): Column to use for annotating the t-SNE plot\n",
    "    batch_size (int): Number of data points to include in the t-SNE plot\n",
    "    show_global (bool): Whether to show global embeddings in the t-SNE plot\n",
    "    video_frame_width (float): Width ratio for the video frames subplot\n",
    "\n",
    "    Returns:\n",
    "    fig (matplotlib.figure.Figure): The created figure\n",
    "    \"\"\"\n",
    "    path2video = f\"/home/dl18206/Desktop/phd/data/panaf/acp/videos/all/{video_name}\"\n",
    "    video_tensor = torchvision.io.read_video(path2video, pts_unit=\"sec\")[0]\n",
    "\n",
    "    logits, global_logits, labels, class_seq = extract_info(df, video_name)\n",
    "\n",
    "    # Apply sigmoid function to logits and global logits\n",
    "    sigmoid_logits = 1 / (1 + np.exp(-logits))\n",
    "    sigmoid_global_logits = 1 / (1 + np.exp(-global_logits))\n",
    "\n",
    "    # Create a custom colormap that goes from white to blue\n",
    "    colors = [(1, 1, 1), (0, 0, 1)]  # White to Blue\n",
    "    n_bins = 100  # Number of color gradations\n",
    "    cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors, N=n_bins)\n",
    "\n",
    "    # Calculate width ratios based on the video_frame_width\n",
    "    width_ratios = [6, 1, 1, 0.4, video_frame_width, 3]\n",
    "    total_width = sum(width_ratios)\n",
    "    width_ratios = [\n",
    "        w / total_width * 30 for w in width_ratios\n",
    "    ]  # Scale to 30 total width\n",
    "\n",
    "    # Create a figure with a custom grid\n",
    "    fig = plt.figure(figsize=(30, 8))\n",
    "    gs = GridSpec(1, 6, figure=fig, width_ratios=width_ratios)\n",
    "\n",
    "    # Subfigure 1: Sigmoid-activated logits\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    im1 = ax1.imshow(sigmoid_logits.T, aspect=\"auto\", cmap=cmap, vmin=0, vmax=1)\n",
    "    ax1.set_xlabel(\"Subclips\")\n",
    "    ax1.set_ylabel(\"Classes\")\n",
    "    ax1.set_title(f\"{video_name}: {class_seq}\")\n",
    "\n",
    "    # Add text annotations for sigmoid logits\n",
    "    for i in range(sigmoid_logits.shape[1]):\n",
    "        for j in range(sigmoid_logits.shape[0]):\n",
    "            ax1.text(\n",
    "                j,\n",
    "                i,\n",
    "                f\"{sigmoid_logits[j, i]:.2f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                color=\"black\",\n",
    "                fontsize=6,\n",
    "            )\n",
    "\n",
    "    # Subfigure 2: Global logits\n",
    "    ax2 = fig.add_subplot(gs[1])\n",
    "    im2 = ax2.imshow(sigmoid_global_logits.T, aspect=\"auto\", cmap=cmap, vmin=0, vmax=1)\n",
    "    ax2.set_title(\"Global Logits\")\n",
    "\n",
    "    # Add text annotations for global logits\n",
    "    for i in range(sigmoid_global_logits.shape[1]):\n",
    "        ax2.text(\n",
    "            0,\n",
    "            i,\n",
    "            f\"{sigmoid_global_logits[0, i]:.2f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=\"black\",\n",
    "            fontsize=6,\n",
    "        )\n",
    "\n",
    "    # Subfigure 3: Multi-hot encoded labels\n",
    "    ax3 = fig.add_subplot(gs[2])\n",
    "    im3 = ax3.imshow(labels.T, aspect=\"auto\", cmap=cmap, vmin=0, vmax=1)\n",
    "    ax3.set_title(\"Multi-hot Encoded Label\")\n",
    "\n",
    "    # Add text annotations for labels\n",
    "    for i in range(labels.shape[1]):\n",
    "        ax3.text(\n",
    "            0,\n",
    "            i,\n",
    "            str(int(labels[0, i])),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=\"black\",\n",
    "            fontsize=6,\n",
    "        )\n",
    "\n",
    "    # Set ticks for subfigures\n",
    "    ax1.set_xticks(np.arange(0, sigmoid_logits.shape[0]))\n",
    "    ax1.set_xticklabels(np.arange(0, sigmoid_logits.shape[0]))\n",
    "    ax1.set_yticks(np.arange(sigmoid_logits.shape[1]))\n",
    "    ax1.set_yticklabels(behaviours)\n",
    "\n",
    "    for ax in [ax2, ax3]:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticks(np.arange(labels.shape[1]))\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "    plt.setp(ax1.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # Add a single colorbar\n",
    "    cax = fig.add_subplot(gs[3])\n",
    "    cbar = fig.colorbar(im1, cax=cax)\n",
    "    cbar.set_label(\"Activation Strength / Label Presence\")\n",
    "\n",
    "    # Subfigure 4: Video frames\n",
    "    ax4 = fig.add_subplot(gs[4])\n",
    "    clips = video_to_clips(\n",
    "        video_tensor, n_clips, n_samples, sliding_window=sliding_window\n",
    "    )\n",
    "    clips = rearrange(torch.stack(clips), \"n t h w c -> (n t) c h w\")\n",
    "    grid = make_grid(clips, nrow=n_samples)\n",
    "    ax4.imshow(grid.permute(1, 2, 0))\n",
    "    ax4.set_title(\"Video Frames\")\n",
    "\n",
    "    # Add y-ticks to video frames subfigure\n",
    "    ax4.set_yticks(\n",
    "        np.linspace(0, grid.shape[1], n_clips + 1)[:-1] + grid.shape[1] / (2 * n_clips)\n",
    "    )\n",
    "    ax4.set_yticklabels(range(n_clips))\n",
    "    ax4.set_ylabel(\"Clip Number\")\n",
    "\n",
    "    # Subfigure 5: t-SNE plot\n",
    "    ax5 = fig.add_subplot(gs[5])\n",
    "    plot_tsne_results_subplot(\n",
    "        ax5,\n",
    "        df=df,\n",
    "        colour_by=colour_by,\n",
    "        annotate_by=annotate_by,\n",
    "        video_name=video_name,\n",
    "        batch_size=batch_size,\n",
    "        show_global=show_global,\n",
    "    )\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    if save_prefix is not None:\n",
    "        plt.savefig(f\"{save_prefix}_v={video_name}.pdf\", dpi=300)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'visualize_logits_labels_video_and_tsne' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-b97af5daa3ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvideo_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     fig = visualize_logits_labels_video_and_tsne(\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mvideo_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'visualize_logits_labels_video_and_tsne' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "videos = [\"acp0000eeb.mp4\", \"acp0000948.mp4\", \"acp00000mc.mp4\"]\n",
    "\n",
    "for video_name in tqdm(videos):\n",
    "    fig = visualize_logits_labels_video_and_tsne(\n",
    "        df,\n",
    "        video_name,\n",
    "        behaviours=behaviours,\n",
    "        n_clips=16,\n",
    "        n_samples=4,\n",
    "        sliding_window=True,\n",
    "        save_prefix=save_name,\n",
    "        colour_by=\"name\",\n",
    "        annotate_by=\"timestep\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataset-upgrade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
