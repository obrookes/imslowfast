{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from plot_utils import (\n",
    "    plot_heatmap,\n",
    "    plot_training_progression,\n",
    "    plot_training_progression_with_variation,\n",
    "    plot_map_values,\n",
    ")\n",
    "from data_utils import (\n",
    "    read_files,\n",
    "    results2df,\n",
    "    calculate_all_metrics,\n",
    "    return_ct_location_segments,\n",
    ")\n",
    "\n",
    "plt.style.use(\"science\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load annotations and results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model name\n",
    "model_name = \"slow_r50\"\n",
    "folder_path = \"../dataset/results/training_progression\"\n",
    "metadata_file = \"../dataset/metadata/metadata.csv\"\n",
    "behavioural_labels_file = \"../dataset/metadata/behaviours.txt\"\n",
    "segements_file = \"../dataset/metadata/segments.txt\"\n",
    "figure_saving_path = \"../figures/training_progression\"\n",
    "\n",
    "\n",
    "splits = [\n",
    "    \"train\",\n",
    "    \"validation\",\n",
    "]\n",
    "\n",
    "# create saving directory\n",
    "if not os.path.exists(figure_saving_path):\n",
    "    os.makedirs(figure_saving_path)\n",
    "\n",
    "\n",
    "# list all result files in the folder which end with .pkl and contain the model name\n",
    "result_info = {}\n",
    "\n",
    "for split in splits:\n",
    "    for file in os.listdir(os.path.join(folder_path, split)):\n",
    "        if file.endswith(\".pkl\") and model_name in file:\n",
    "            epoch = file.split(\"_\")[-2].split(\"-\")[-1]\n",
    "\n",
    "            # get the split from the file name\n",
    "            data_split = file.split(\"=\")[-1].split(\".\")[0]\n",
    "\n",
    "            # add model to the dictionary\n",
    "            if model_name not in result_info:\n",
    "                result_info[model_name] = {}\n",
    "            # add epoch to the dictionary\n",
    "            if epoch not in result_info[model_name]:\n",
    "                result_info[model_name][epoch] = {}\n",
    "            if split not in result_info[model_name][epoch]:\n",
    "                result_info[model_name][epoch][data_split] = {}\n",
    "            result_info[model_name][epoch][data_split] = {\n",
    "                \"file_path\": os.path.join(folder_path, split, file),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_csv(metadata_file)\n",
    "\n",
    "with open(behavioural_labels_file, \"rb\") as f:\n",
    "    behaviours = [beh.decode(\"utf-8\").strip() for beh in f.readlines()]\n",
    "\n",
    "with open(segements_file, \"rb\") as f:\n",
    "    segments = [seg.decode(\"utf-8\").strip() for seg in f.readlines()]\n",
    "\n",
    "# build dict for behavioural where key is the segment and value is the behaviour is the same index\n",
    "behavioural_dict = {}\n",
    "for i, (b, s) in enumerate(zip(behaviours, segments)):\n",
    "    if s not in behavioural_dict:\n",
    "        behavioural_dict[s] = []\n",
    "    behavioural_dict[s].append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Head\", \"Tail\", \"Few-shot\"]\n",
    "reordered_behaviours = [\n",
    "    \"resting\",\n",
    "    \"travel\",\n",
    "    \"bipedal\",\n",
    "    \"camera_reaction\",\n",
    "    \"climbing\",\n",
    "    \"feeding\",\n",
    "    \"grooming\",\n",
    "    \"object_carrying\",\n",
    "    \"tool_use\",\n",
    "    \"vocalisation\",\n",
    "    \"aggression\",\n",
    "    \"display\",\n",
    "    \"piloerection\",\n",
    "    \"playing\",\n",
    "]\n",
    "\n",
    "\n",
    "# Merge behavior data into final_train_data\n",
    "# \"h\", \"t\", \"f\" are the segments of camera locations\n",
    "# \"head\", \"tail\", \"few_shot\" are the segments of behaviors\n",
    "# so if we have a key \"map_h_head_values\" it means that the values are for the head segment of the camera locations and the head segment of the behaviors\n",
    "# Here we aggregate the values of the behaviors for each segment of the camera locations\n",
    "key_map = {\n",
    "    \"map_h_head_values\": \"head\",\n",
    "    \"map_h_tail_values\": \"head\",\n",
    "    \"map_h_fs_values\": \"head\",\n",
    "    \"map_t_head_values\": \"tail\",\n",
    "    \"map_t_tail_values\": \"tail\",\n",
    "    \"map_t_fs_values\": \"tail\",\n",
    "    \"map_f_head_values\": \"few_shot\",\n",
    "    \"map_f_tail_values\": \"few_shot\",\n",
    "    \"map_f_fs_values\": \"few_shot\",\n",
    "}\n",
    "\n",
    "datas = {}\n",
    "\n",
    "\n",
    "for m in result_info:\n",
    "    for epoch in result_info[m]:\n",
    "        train_data, val_data = read_files(result_info[model_name], epoch)\n",
    "        train_df, val_df = results2df(train_data, val_data, metadata_df)\n",
    "\n",
    "        train_segments = {}\n",
    "        val_segments = {}\n",
    "\n",
    "        # Modified segment calculations\n",
    "        th_df, tt_df, tf_df = return_ct_location_segments(train_df, head=50, tail=10)\n",
    "\n",
    "        vh_df = val_df[val_df[\"utm\"].isin(th_df[\"utm\"])]\n",
    "        vt_df = val_df[val_df[\"utm\"].isin(tt_df[\"utm\"])]\n",
    "        vf_df = val_df[val_df[\"utm\"].isin(tf_df[\"utm\"])]\n",
    "\n",
    "        train_segments[\"h\"] = th_df.merge(train_df, on=\"utm\", how=\"left\").dropna()\n",
    "        train_segments[\"t\"] = tt_df.merge(train_df, on=\"utm\", how=\"left\").dropna()\n",
    "        train_segments[\"f\"] = tf_df.merge(train_df, on=\"utm\", how=\"left\").dropna()\n",
    "\n",
    "        val_segments[\"h\"] = vh_df\n",
    "        val_segments[\"t\"] = vt_df\n",
    "        val_segments[\"f\"] = vf_df\n",
    "\n",
    "        # Original segment calculation\n",
    "        # train_segments = process_and_merge(train_df, train_segments)\n",
    "        # val_segments = process_and_merge(val_df, val_segments)\n",
    "\n",
    "        train_metrics = calculate_all_metrics(\n",
    "            train_segments, behaviours, segments, show_per_class=True\n",
    "        )\n",
    "        val_metrics = calculate_all_metrics(\n",
    "            val_segments, behaviours, segments, show_per_class=True\n",
    "        )\n",
    "\n",
    "        train_data = np.array(\n",
    "            [\n",
    "                [\n",
    "                    train_metrics[\"map_h_head\"],\n",
    "                    train_metrics[\"map_h_tail\"],\n",
    "                    train_metrics[\"map_h_fs\"],\n",
    "                ],\n",
    "                [\n",
    "                    train_metrics[\"map_t_head\"],\n",
    "                    train_metrics[\"map_t_tail\"],\n",
    "                    train_metrics[\"map_t_fs\"],\n",
    "                ],\n",
    "                [\n",
    "                    train_metrics[\"map_f_head\"],\n",
    "                    train_metrics[\"map_f_tail\"],\n",
    "                    train_metrics[\"map_f_fs\"],\n",
    "                ],\n",
    "            ]\n",
    "        ).T\n",
    "\n",
    "        val_data = np.array(\n",
    "            [\n",
    "                [\n",
    "                    val_metrics[\"map_h_head\"],\n",
    "                    val_metrics[\"map_h_tail\"],\n",
    "                    val_metrics[\"map_h_fs\"],\n",
    "                ],\n",
    "                [\n",
    "                    val_metrics[\"map_t_head\"],\n",
    "                    val_metrics[\"map_t_tail\"],\n",
    "                    val_metrics[\"map_t_fs\"],\n",
    "                ],\n",
    "                [\n",
    "                    val_metrics[\"map_f_head\"],\n",
    "                    val_metrics[\"map_f_tail\"],\n",
    "                    val_metrics[\"map_f_fs\"],\n",
    "                ],\n",
    "            ]\n",
    "        ).T\n",
    "\n",
    "        # plot for each behaviour the performance for head, tail and few-shot segments\n",
    "\n",
    "        # Extract and merge behavior data\n",
    "        train_behaviour_data = defaultdict(dict)\n",
    "        val_behaviour_data = defaultdict(dict)\n",
    "\n",
    "        for key in key_map.keys():\n",
    "            v = [list(d.values())[0] for d in train_metrics[key]]\n",
    "            k = [list(d.keys())[0] for d in train_metrics[key]]\n",
    "            # print(f\"train mAP {key}: {sum(v) / len(v)}\")\n",
    "            for i, value in enumerate(v):\n",
    "                train_behaviour_data[key][k[i]] = value\n",
    "\n",
    "            v = [list(d.values())[0] for d in val_metrics[key]]\n",
    "            k = [list(d.keys())[0] for d in val_metrics[key]]\n",
    "            # print(f\"val mAP for {key}: {sum(v) / len(v)}\")\n",
    "            for i, value in enumerate(v):\n",
    "                val_behaviour_data[key][k[i]] = value\n",
    "\n",
    "        # Initialize final_train_data with keys head, tail, few_shot\n",
    "        final_train_data = {seg: {} for seg in set(list(key_map.values()))}\n",
    "        final_val_data = {seg: {} for seg in set(list(key_map.values()))}\n",
    "\n",
    "        for k, v in train_behaviour_data.items():\n",
    "            if k in key_map:\n",
    "                final_train_data[key_map[k]].update(v)\n",
    "\n",
    "        for k, v in val_behaviour_data.items():\n",
    "            if k in key_map:\n",
    "                final_val_data[key_map[k]].update(v)\n",
    "\n",
    "        # Prepare data matrix\n",
    "        train_data_matrix = np.array(\n",
    "            [\n",
    "                [final_train_data[\"head\"][beh] for beh in reordered_behaviours],\n",
    "                [final_train_data[\"tail\"][beh] for beh in reordered_behaviours],\n",
    "                [final_train_data[\"few_shot\"][beh] for beh in reordered_behaviours],\n",
    "            ]\n",
    "        ).T\n",
    "        val_data_matrix = np.array(\n",
    "            [\n",
    "                [final_val_data[\"head\"][beh] for beh in reordered_behaviours],\n",
    "                [final_val_data[\"tail\"][beh] for beh in reordered_behaviours],\n",
    "                [final_val_data[\"few_shot\"][beh] for beh in reordered_behaviours],\n",
    "            ]\n",
    "        ).T\n",
    "\n",
    "        # add epoch to the dictionary\n",
    "        if epoch not in datas.keys():\n",
    "            datas[epoch] = {\n",
    "                \"train_data\": {},\n",
    "                \"val_data\": {},\n",
    "                \"train_data_matrix\": {},\n",
    "                \"val_data_matrix\": {},\n",
    "            }\n",
    "\n",
    "        # add data to the dictionary\n",
    "        datas[epoch][\"train_data\"] = train_data\n",
    "        datas[epoch][\"val_data\"] = val_data\n",
    "        datas[epoch][\"train_data_matrix\"] = train_data_matrix\n",
    "        datas[epoch][\"val_data_matrix\"] = val_data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in datas.keys():\n",
    "    train_hmap = datas[epoch][\"train_data\"][:, 0].mean()\n",
    "    val_hmap = datas[epoch][\"val_data\"][:, 0].mean()\n",
    "\n",
    "    train_tmap = datas[epoch][\"train_data\"][:, 1].mean()\n",
    "    val_tmap = datas[epoch][\"val_data\"][:, 1].mean()\n",
    "\n",
    "    train_fsmap = datas[epoch][\"train_data\"][:, 2].mean()\n",
    "    val_fsmap = datas[epoch][\"val_data\"][:, 2].mean()\n",
    "\n",
    "    datas[epoch][\"map\"] = {\n",
    "        \"train_head_loc\": train_hmap,\n",
    "        \"val_head_loc\": val_hmap,\n",
    "        \"train_tail_loc\": train_tmap,\n",
    "        \"val_tail_loc\": val_tmap,\n",
    "        \"train_few_shot_loc\": train_fsmap,\n",
    "        \"val_few_shot_loc\": val_fsmap,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_oder = [int(e) for e in datas.keys()]\n",
    "\n",
    "# sort the dictionary by epoch\n",
    "datas = dict(sorted(datas.items(), key=lambda item: int(item[0])))\n",
    "\n",
    "print(datas.keys())\n",
    "\n",
    "# plot the heatmap for each epoch for train split in first row and val split in the second row\n",
    "fig, ax = plt.subplots(2, len(datas.keys()), figsize=(29, 17), sharey=False, dpi=300)\n",
    "\n",
    "for i, epoch in enumerate(datas.keys()):\n",
    "    plot_heatmap(\n",
    "        data=datas[epoch][\"train_data_matrix\"],\n",
    "        behavioural_dict=behavioural_dict,\n",
    "        title=f\"Train, epoch: {epoch}, model: {model_name}\",\n",
    "        ax=ax[0, i],\n",
    "        labels=labels,\n",
    "        behaviors=reordered_behaviours,\n",
    "    )\n",
    "\n",
    "    plot_heatmap(\n",
    "        data=datas[epoch][\"val_data_matrix\"],\n",
    "        behavioural_dict=behavioural_dict,\n",
    "        title=f\"Val, epoch: {epoch}, model: {model_name}\",\n",
    "        ax=ax[1, i],\n",
    "        labels=labels,\n",
    "        behaviors=reordered_behaviours,\n",
    "    )\n",
    "\n",
    "# save as png\n",
    "fig.savefig(\n",
    "    os.path.join(figure_saving_path, f\"heatmap_{model_name}_behaviorswise.png\"),\n",
    "    dpi=300,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the heatmap for each epoch for train split in first row and val split in the second row\n",
    "fig, ax = plt.subplots(2, len(datas.keys()), figsize=(20, 5), sharey=False, dpi=200)\n",
    "\n",
    "for i, epoch in enumerate(datas.keys()):\n",
    "    plot_heatmap(\n",
    "        datas[epoch][\"train_data\"],\n",
    "        behavioural_dict,\n",
    "        f\"Train, epoch: {epoch}, model: {model_name}\",\n",
    "        ax[0, i],\n",
    "        labels,\n",
    "    )\n",
    "\n",
    "    plot_heatmap(\n",
    "        datas[epoch][\"val_data\"],\n",
    "        behavioural_dict,\n",
    "        f\"Val, epoch: {epoch}, model: {model_name}\",\n",
    "        ax[1, i],\n",
    "        labels,\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "# save as png\n",
    "fig.savefig(\n",
    "    os.path.join(figure_saving_path, f\"heatmap_{model_name}.png\"),\n",
    "    dpi=200,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_progression(datas, split=\"train_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_progression(datas, split=\"val_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_progression_with_variation(datas, split=\"train_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_progression_with_variation(datas, split=\"val_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map_values(datas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataset-upgrade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
