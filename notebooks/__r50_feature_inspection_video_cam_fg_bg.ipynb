{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Plotting imports\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Slowfast imports\n",
    "from slowfast.models import build_model\n",
    "from slowfast.utils.checkpoint import load_checkpoint\n",
    "from slowfast.utils.parser import load_config, alt_parse_args\n",
    "from slowfast.datasets.loader import construct_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviours_file = \"../dataset/metadata/behaviours.txt\"\n",
    "\n",
    "with open(behaviours_file, \"rb\") as f:\n",
    "    behaviours = [beh.decode(\"utf-8\").strip() for beh in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_model_path = \"/home/dl18206/Desktop/phd/code/personal/facebook/slowfast/dataset/weights/bg-only/model=slow_r50_bg-only_e300.pyth\"\n",
    "bl_model_path = \"/home/dl18206/Desktop/phd/code/personal/facebook/slowfast/dataset/weights/bg-only/model=slow_r50_baseline_e300.pyth\"\n",
    "# \"/home/dl18206/Desktop/phd/code/personal/facebook/slowfast/dataset/weights/model=slow-r50_w-negatives_epoch=001.pyth\"\n",
    "# r50_model_path = (\n",
    "#     \"/home/dl18206/Desktop/phd/code/personal/facebook/slowfast/SLOWONLY_8x8_R50.pkl\"\n",
    "# )\n",
    "\n",
    "cfg_path = \"/home/dl18206/Desktop/phd/code/personal/facebook/slowfast/configs/SLOW_8x8_R50_Local.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alt_load_config(cfg_path):\n",
    "    args = alt_parse_args()[:-1]\n",
    "    cfg = load_config(\n",
    "        args[0],\n",
    "        path_to_config=cfg_path,\n",
    "    )\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(cfg_path, ckpt_path, map_location=\"cpu\", ckpt_type=\"pytorch\"):\n",
    "    # Load model config\n",
    "    cfg = alt_load_config(cfg_path)\n",
    "    model = build_model(cfg)\n",
    "    if ckpt_type == \"pytorch\":\n",
    "        checkpoint = torch.load(ckpt_path, map_location=map_location)\n",
    "        model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    elif ckpt_type == \"caffe2\":\n",
    "        load_checkpoint(\n",
    "            path_to_checkpoint=ckpt_path,\n",
    "            model=model,\n",
    "            convert_from_caffe2=True,\n",
    "            data_parallel=False,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Invalid checkpoint type. Choose 'pytorch' or 'caffe2'\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_maps(model, inputs):\n",
    "    model.eval()\n",
    "    # Ensure model and inputs are on cuda if available\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        inputs[0] = inputs[0].cuda()\n",
    "    with torch.no_grad():\n",
    "        return model.s5(model.s4(model.s3(model.s2(model.s1([inputs[0]])))))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_frames(video_name):\n",
    "    # Get original video\n",
    "    video_path = \"/home/dl18206/Desktop/phd/data/panaf/panaf_sequence\"\n",
    "\n",
    "    batch_frames = []\n",
    "\n",
    "    for path in video_name:\n",
    "        cap = cv2.VideoCapture(os.path.join(video_path, path))\n",
    "        frames = []\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, (256, 256))\n",
    "            frames.append(frame)\n",
    "\n",
    "        batch_frames.append(np.stack(frames))\n",
    "\n",
    "    return np.stack(batch_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_spatial_cam(\n",
    "    feature_conv, weight_softmax, class_idx, size_upsample=(256, 256), normalise=False\n",
    "):\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h * w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        # Min max normalization\n",
    "        if normalise:\n",
    "            cam = cam - np.min(cam)\n",
    "            cam_img = cam / np.max(cam)\n",
    "            cam_img = np.uint8(255 * cam_img)\n",
    "            cam = cam_img\n",
    "        if size_upsample is not None:\n",
    "            cam = cv2.resize(cam, size_upsample)\n",
    "        output_cam.append(cam)\n",
    "    return np.stack(output_cam)\n",
    "\n",
    "\n",
    "def return_spatio_temporal_cam(\n",
    "    feature_conv,\n",
    "    weight_softmax,\n",
    "    class_idx,\n",
    "    size_upsample=(256, 256),\n",
    "    normalise=False,\n",
    "):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    bsz, nc, t, h, w = feature_conv.shape\n",
    "    batch_cam = []\n",
    "    for batch in range(bsz):\n",
    "        spatio_temporal_cam = []\n",
    "        for t_step in range(t):\n",
    "            spatial_cam = []\n",
    "            spatial_conv = feature_conv[batch, :, t_step, :, :].unsqueeze(0)\n",
    "            spatial_cam = return_spatial_cam(\n",
    "                spatial_conv, weight_softmax, class_idx, size_upsample, normalise\n",
    "            )\n",
    "            spatio_temporal_cam.append(spatial_cam)\n",
    "        batch_cam.append(spatio_temporal_cam)\n",
    "    return np.stack(batch_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frames_grid(frames, batch_index=0, start_frame=0, title=None):\n",
    "    \"\"\"\n",
    "    Plot frames in a 4x4 grid.\n",
    "\n",
    "    :param frames: Array of shape (b, t, h, w, c)\n",
    "    :param batch_index: Index of the batch to use (default 0)\n",
    "    :param start_frame: Starting frame index (default 0)\n",
    "    :param title: Title for the entire figure (optional)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(15, 15))\n",
    "    fig.suptitle(title, fontsize=16, y=1.02) if title else None\n",
    "\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            frame_index = start_frame + i * 4 + j\n",
    "            if frame_index < frames.shape[1]:\n",
    "                frame = frames[batch_index, frame_index]\n",
    "\n",
    "                axes[i, j].imshow(frame)\n",
    "                axes[i, j].axis(\"off\")\n",
    "                axes[i, j].set_title(f\"Frame {frame_index}\")\n",
    "            else:\n",
    "                axes[i, j].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_model = load_model(cfg_path, bg_model_path)\n",
    "bl_model = load_model(cfg_path, bl_model_path)\n",
    "\n",
    "bg_classifier = bg_model.head.projection\n",
    "bl_classifier = bl_model.head.projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nkinetics_cfg_path = \"/home/dl18206/Desktop/phd/code/personal/facebook/slowfast/configs/SLOW_8x8_R50_FGBG_Local.yaml\"\n",
    "loader = construct_loader(alt_load_config(nkinetics_cfg_path), \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels, index, time, meta = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assert spatial and spatio-temporal CAMs are equivalent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk through CAM calculation and logit generation\n",
    "batch_frames = get_video_frames(meta[\"fg_video_name\"])\n",
    "\n",
    "fg_feature_maps = get_feature_maps(bl_model, inputs[\"fg_frames\"])\n",
    "bg_feature_maps = get_feature_maps(bg_model, inputs[\"fg_frames\"])\n",
    "\n",
    "# Spatial feature map for frame 0\n",
    "fg_spatial_map = fg_feature_maps[:, :, 0, :, :]\n",
    "\n",
    "fg_spatial_cams = return_spatial_cam(\n",
    "    fg_spatial_map.detach(),\n",
    "    bg_classifier.weight.detach().numpy(),\n",
    "    torch.linspace(0, 13, steps=14).int(),\n",
    ")\n",
    "\n",
    "fg_spatio_temporal_cams = return_spatio_temporal_cam(\n",
    "    fg_feature_maps.detach(),\n",
    "    bg_classifier.weight.detach().numpy(),\n",
    "    torch.linspace(0, 13, steps=14).int(),\n",
    ").squeeze(0)\n",
    "\n",
    "print(fg_spatial_cams.shape, fg_spatio_temporal_cams.shape)\n",
    "\n",
    "assert np.all(fg_spatial_cams[0] == fg_spatio_temporal_cams[0][0])\n",
    "\n",
    "fg_spatial_logits = (\n",
    "    F.adaptive_avg_pool2d(torch.tensor(fg_spatial_cams), 1).squeeze(-1).squeeze(-1)\n",
    ")\n",
    "\n",
    "# Only for the first frame\n",
    "fg_spatio_temporal_logits = (\n",
    "    F.adaptive_avg_pool2d(torch.tensor(fg_spatio_temporal_cams[0]), 1)\n",
    "    .squeeze(-1)\n",
    "    .squeeze(-1)\n",
    ")\n",
    "\n",
    "assert np.all(fg_spatial_logits.numpy() == fg_spatio_temporal_logits.numpy())\n",
    "\n",
    "# we can interpret each element as an attribution score at (t, h, w) contributing to the logit\n",
    "fg_spatio_temporal_cams = rearrange(\n",
    "    torch.tensor(fg_spatio_temporal_cams).unsqueeze(0), \"b t c h w -> b c t h w\"\n",
    ")\n",
    "\n",
    "logits = (\n",
    "    F.adaptive_avg_pool3d(torch.tensor(fg_spatio_temporal_cams.squeeze(0)), 1)\n",
    "    .squeeze(-1)\n",
    "    .squeeze(-1)\n",
    "    .squeeze(-1)\n",
    ")\n",
    "\n",
    "assert len(logits) == 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_framewise_features(feature_map, t):\n",
    "    spatially_pooled = F.adaptive_avg_pool3d(feature_map, (t, 1, 1))\n",
    "    frame_wise_features = torch.flatten(spatially_pooled, start_dim=2)\n",
    "    return frame_wise_features\n",
    "\n",
    "\n",
    "def get_framewise_logits(framewise_features, classifier):\n",
    "    frame_wise_logits = []\n",
    "    for feat in framewise_features:\n",
    "        frame_wise_logits.append((classifier(feat).detach()))\n",
    "    return torch.stack(frame_wise_logits)\n",
    "\n",
    "\n",
    "video_features = F.adaptive_avg_pool3d(fg_feature_maps, (1, 1, 1))\n",
    "video_features = torch.flatten(video_features, start_dim=1)\n",
    "video_logits = torch.sigmoid(bl_classifier(video_features)).detach().numpy()\n",
    "\n",
    "framewise_features = extract_framewise_features(fg_feature_maps, t=16)\n",
    "framewise_logits = get_framewise_logits(\n",
    "    framewise_features.permute(0, 2, 1), bl_classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot CAMs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = 24\n",
    "all_behaviours = False\n",
    "normalise = False\n",
    "size_upsample = (256, 256)\n",
    "\n",
    "for inputs, labels, index, time, meta in loader:\n",
    "    # Get video frames\n",
    "    fg_batch_frames = get_video_frames(meta[\"fg_video_name\"])\n",
    "    bg_batch_frames = get_video_frames(meta[\"bg_video_name\"])\n",
    "\n",
    "    # Get behaviours from the labels\n",
    "    if all_behaviours:\n",
    "        label_idxs = range(len(behaviours))\n",
    "    else:\n",
    "        label_idxs = labels[0].nonzero().squeeze().tolist()\n",
    "        if isinstance(label_idxs, int):\n",
    "            label_idxs = [label_idxs]\n",
    "\n",
    "    print([behaviours[idx] for idx in label_idxs])\n",
    "\n",
    "    bl_fg_feature_maps = get_feature_maps(bl_model, inputs[\"fg_frames\"])\n",
    "    bl_bg_feature_maps = get_feature_maps(bl_model, inputs[\"bg_frames\"])\n",
    "\n",
    "    bg_fg_feature_maps = get_feature_maps(bg_model, inputs[\"fg_frames\"])\n",
    "    bg_bg_feature_maps = get_feature_maps(bg_model, inputs[\"bg_frames\"])\n",
    "\n",
    "    for tensor_idx in range(14):\n",
    "\n",
    "        frame_idx = 0 if tensor_idx == 0 else tensor_idx * fps\n",
    "\n",
    "        bl_fg_spatial_map = bl_fg_feature_maps[\n",
    "            :, :, tensor_idx, :, :\n",
    "        ]  # bsz must be equal to 1\n",
    "\n",
    "        bl_bg_spatial_map = bl_bg_feature_maps[\n",
    "            :, :, tensor_idx, :, :\n",
    "        ]  # bsz must be equal to 1\n",
    "\n",
    "        bg_fg_spatial_map = bg_fg_feature_maps[\n",
    "            :, :, tensor_idx, :, :\n",
    "        ]  # bsz must be equal to 1\n",
    "\n",
    "        bg_bg_spatial_map = bg_bg_feature_maps[\n",
    "            :, :, tensor_idx, :, :\n",
    "        ]  # bsz must be equal to 1\n",
    "\n",
    "        bl_fg_cams = return_spatial_cam(\n",
    "            bl_fg_spatial_map.detach().cpu(),\n",
    "            bg_classifier.weight.detach().cpu().numpy(),\n",
    "            torch.linspace(0, 13, steps=14).int(),\n",
    "            normalise=normalise,\n",
    "            size_upsample=size_upsample,\n",
    "        )\n",
    "\n",
    "        bl_bg_cams = return_spatial_cam(\n",
    "            bl_bg_spatial_map.detach().cpu(),\n",
    "            bg_classifier.weight.detach().cpu().numpy(),\n",
    "            torch.linspace(0, 13, steps=14).int(),\n",
    "            normalise=normalise,\n",
    "            size_upsample=size_upsample,\n",
    "        )\n",
    "\n",
    "        bg_fg_cams = return_spatial_cam(\n",
    "            bg_fg_spatial_map.detach().cpu(),\n",
    "            bg_classifier.weight.detach().cpu().numpy(),\n",
    "            torch.linspace(0, 13, steps=14).int(),\n",
    "            normalise=normalise,\n",
    "            size_upsample=size_upsample,\n",
    "        )\n",
    "\n",
    "        bg_bg_cams = return_spatial_cam(\n",
    "            bg_bg_spatial_map.detach().cpu(),\n",
    "            bg_classifier.weight.detach().cpu().numpy(),\n",
    "            torch.linspace(0, 13, steps=14).int(),\n",
    "            normalise=normalise,\n",
    "            size_upsample=size_upsample,\n",
    "        )\n",
    "\n",
    "        for l in label_idxs:\n",
    "            behaviour_idx = behaviours.index(behaviours[l])\n",
    "\n",
    "            # Increase the figure height to accommodate the main title\n",
    "            fig, ax = plt.subplots(1, 6, figsize=(30, 4))\n",
    "\n",
    "            # Baseline Model heatmap\n",
    "            bl_fg_cam = bl_fg_cams[behaviour_idx]\n",
    "            sns.heatmap(bl_fg_cam, ax=ax[0], cbar=True, cmap=\"viridis\")\n",
    "            ax[0].set_title(\"BL FG\")\n",
    "\n",
    "            bl_bg_cam = bl_bg_cams[behaviour_idx]\n",
    "            sns.heatmap(bl_bg_cam, ax=ax[1], cbar=True, cmap=\"viridis\")\n",
    "            ax[1].set_title(\"BL BG\")\n",
    "\n",
    "            bg_fg_cam = bg_fg_cams[behaviour_idx]\n",
    "            sns.heatmap(bg_fg_cam, ax=ax[2], cbar=True, cmap=\"viridis\")\n",
    "            ax[2].set_title(\"BG FG\")\n",
    "\n",
    "            bg_bg_cam = bg_bg_cams[behaviour_idx]\n",
    "            sns.heatmap(bg_bg_cam, ax=ax[3], cbar=True, cmap=\"viridis\")\n",
    "            ax[3].set_title(\"BG BG\")\n",
    "\n",
    "            # Plot the bl model on fg frame\n",
    "            fg_frame = fg_batch_frames[0][frame_idx]\n",
    "            fg_frame = cv2.resize(fg_frame, size_upsample)\n",
    "            height, width, _ = fg_frame.shape\n",
    "            bl_fg_cam = cv2.applyColorMap(\n",
    "                cv2.resize(bl_fg_cam.astype(np.uint8), (width, height)),\n",
    "                cv2.COLORMAP_JET,\n",
    "            )\n",
    "            result = bl_fg_cam * 0.3 + fg_frame * 0.5\n",
    "            ax[4].imshow((result).astype(np.uint8))\n",
    "            ax[4].set_title(f\"BL Model on FG {frame_idx}\")\n",
    "\n",
    "            # Plot the bl model on bg frame\n",
    "            bg_frame = bg_batch_frames[0][frame_idx]\n",
    "            bg_frame = cv2.resize(bg_frame, size_upsample)\n",
    "            height, width, _ = bg_frame.shape\n",
    "            bl_bg_cam = cv2.applyColorMap(\n",
    "                cv2.resize(bl_bg_cam.astype(np.uint8), (width, height)),\n",
    "                cv2.COLORMAP_JET,\n",
    "            )\n",
    "            result = bl_bg_cam * 0.3 + bg_frame * 0.5\n",
    "            ax[5].imshow((result).astype(np.uint8))\n",
    "            ax[5].set_title(f\"BL Model on BG {frame_idx}\")\n",
    "\n",
    "            # Add the main title with increased spacing\n",
    "            plt.suptitle(f\"{behaviours[l]}\", fontsize=16, y=1.05)\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate attribution score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_fg_store, bl_bg_store, bg_fg_store, bg_bg_store = [], [], [], []\n",
    "fg_names, bg_names = [], []\n",
    "\n",
    "for inputs, labels, index, time, meta in tqdm(loader):\n",
    "\n",
    "    # Get video frames\n",
    "    fg_batch_frames = get_video_frames(meta[\"fg_video_name\"])\n",
    "    bg_batch_frames = get_video_frames(meta[\"bg_video_name\"])\n",
    "\n",
    "    bl_fg_feature_maps = get_feature_maps(bl_model, inputs[\"fg_frames\"])\n",
    "    bl_bg_feature_maps = get_feature_maps(bl_model, inputs[\"bg_frames\"])\n",
    "\n",
    "    bg_fg_feature_maps = get_feature_maps(bg_model, inputs[\"fg_frames\"])\n",
    "    bg_bg_feature_maps = get_feature_maps(bg_model, inputs[\"bg_frames\"])\n",
    "\n",
    "    # Get spatio-temporal CAMs\n",
    "    bl_fg_spatio_temporal_cams = return_spatio_temporal_cam(\n",
    "        bl_fg_feature_maps.detach().cpu(),\n",
    "        bl_classifier.weight.detach().cpu().numpy(),\n",
    "        torch.linspace(0, 13, steps=14).int(),\n",
    "        normalise=False,\n",
    "        size_upsample=None,\n",
    "    ).squeeze(0)\n",
    "\n",
    "    bl_bg_spatio_temporal_cams = return_spatio_temporal_cam(\n",
    "        bl_bg_feature_maps.detach().detach().cpu(),\n",
    "        bl_classifier.weight.detach().cpu().numpy(),\n",
    "        torch.linspace(0, 13, steps=14).int(),\n",
    "        normalise=False,\n",
    "        size_upsample=None,\n",
    "    ).squeeze(0)\n",
    "\n",
    "    bg_fg_spatio_temporal_cams = return_spatio_temporal_cam(\n",
    "        bg_fg_feature_maps.detach().cpu(),\n",
    "        bg_classifier.weight.detach().cpu().numpy(),\n",
    "        torch.linspace(0, 13, steps=14).int(),\n",
    "        normalise=False,\n",
    "        size_upsample=None,\n",
    "    ).squeeze(0)\n",
    "\n",
    "    bg_bg_spatio_temporal_cams = return_spatio_temporal_cam(\n",
    "        bg_bg_feature_maps.detach().cpu(),\n",
    "        bg_classifier.weight.detach().cpu().numpy(),\n",
    "        torch.linspace(0, 13, steps=14).int(),\n",
    "        normalise=False,\n",
    "        size_upsample=None,\n",
    "    ).squeeze(0)\n",
    "\n",
    "    # we can interpret each element as an attribution score at (t, h, w) contributing to the logit\n",
    "    bl_fg_spatio_temporal_cams = rearrange(\n",
    "        torch.tensor(bl_fg_spatio_temporal_cams).unsqueeze(0), \"b t c h w -> b c t h w\"\n",
    "    )\n",
    "\n",
    "    bl_bg_spatio_temporal_cams = rearrange(\n",
    "        torch.tensor(bl_bg_spatio_temporal_cams).unsqueeze(0), \"b t c h w -> b c t h w\"\n",
    "    )\n",
    "\n",
    "    bg_fg_spatio_temporal_cams = rearrange(\n",
    "        torch.tensor(bg_fg_spatio_temporal_cams).unsqueeze(0), \"b t c h w -> b c t h w\"\n",
    "    )\n",
    "\n",
    "    bg_bg_spatio_temporal_cams = rearrange(\n",
    "        torch.tensor(bg_bg_spatio_temporal_cams).unsqueeze(0), \"b t c h w -> b c t h w\"\n",
    "    )\n",
    "\n",
    "    bl_fg_logits = (\n",
    "        F.adaptive_avg_pool3d(torch.tensor(bl_fg_spatio_temporal_cams.squeeze(0)), 1)\n",
    "        .squeeze(-1)\n",
    "        .squeeze(-1)\n",
    "        .squeeze(-1)\n",
    "    )\n",
    "\n",
    "    bl_bg_logits = (\n",
    "        F.adaptive_avg_pool3d(torch.tensor(bl_bg_spatio_temporal_cams.squeeze(0)), 1)\n",
    "        .squeeze(-1)\n",
    "        .squeeze(-1)\n",
    "        .squeeze(-1)\n",
    "    )\n",
    "\n",
    "    bg_fg_logits = (\n",
    "        F.adaptive_avg_pool3d(torch.tensor(bg_fg_spatio_temporal_cams.squeeze(0)), 1)\n",
    "        .squeeze(-1)\n",
    "        .squeeze(-1)\n",
    "        .squeeze(-1)\n",
    "    )\n",
    "\n",
    "    bg_bg_logits = (\n",
    "        F.adaptive_avg_pool3d(torch.tensor(bg_bg_spatio_temporal_cams.squeeze(0)), 1)\n",
    "        .squeeze(-1)\n",
    "        .squeeze(-1)\n",
    "        .squeeze(-1)\n",
    "    )\n",
    "\n",
    "    bl_fg_store.append(bl_fg_logits.numpy())\n",
    "    bl_bg_store.append(bl_bg_logits.numpy())\n",
    "    bg_fg_store.append(bg_fg_logits.numpy())\n",
    "    bg_bg_store.append(bg_bg_logits.numpy())\n",
    "\n",
    "    fg_names.append(meta[\"fg_video_name\"])\n",
    "    bg_names.append(meta[\"bg_video_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_fg_df = pd.DataFrame(np.vstack(bl_fg_store), columns=behaviours)\n",
    "bl_bg_df = pd.DataFrame(np.vstack(bl_bg_store), columns=behaviours)\n",
    "bg_fg_df = pd.DataFrame(np.vstack(bg_fg_store), columns=behaviours)\n",
    "bg_bg_df = pd.DataFrame(np.vstack(bg_bg_store), columns=behaviours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming fg_cams, bg_cams, and batch_frames are already defined\n",
    "num_cams = len(fg_cams)\n",
    "\n",
    "# Define the desired size for each subplot in pixels\n",
    "subplot_width_px = 256 * 0.5\n",
    "subplot_height_px = 256 * 0.5\n",
    "\n",
    "# Convert pixel size to inches (assuming 100 pixels per inch, adjust if your display has a different DPI)\n",
    "dpi = 100\n",
    "subplot_width_in = subplot_width_px / dpi\n",
    "subplot_height_in = subplot_height_px / dpi\n",
    "\n",
    "# Create a figure with num_cams rows and 3 columns\n",
    "fig, axes = plt.subplots(\n",
    "    num_cams, 3, figsize=(3 * subplot_width_in, num_cams * subplot_height_in), dpi=dpi\n",
    ")\n",
    "\n",
    "for i in range(num_cams):\n",
    "    # If there's only one row, axes[i] will be a 1D array, so we need to handle this case\n",
    "    if num_cams == 1:\n",
    "        current_axes = axes\n",
    "    else:\n",
    "        current_axes = axes[i]\n",
    "\n",
    "    # Plot Baseline Model CAM\n",
    "    sns.heatmap(fg_cams[i], ax=current_axes[0], cbar=True)\n",
    "    current_axes[0].set_title(f\"Baseline Model\")\n",
    "\n",
    "    # Plot Background Only Model CAM\n",
    "    sns.heatmap(bg_cams[i], ax=current_axes[1], cbar=True)\n",
    "    current_axes[1].set_title(f\"Background-Only Model\")\n",
    "\n",
    "    # Plot Original Frame\n",
    "    frame = batch_frames[0][frame_idx]\n",
    "    frame = cv2.resize(frame, (256, 256))\n",
    "    current_axes[2].imshow(frame)\n",
    "    current_axes[2].set_title(f\"Frame {behaviours[i]}\")\n",
    "    current_axes[2].axis(\"off\")\n",
    "\n",
    "# Plot title\n",
    "# plt.suptitle(f\"{}\", y=1.02)\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnCAM_batch(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "\n",
    "    # Reshape feature_conv to (bz, nc, h*w) for batch processing\n",
    "    feature_conv_reshaped = feature_conv.reshape((bz, nc, h * w))\n",
    "\n",
    "    for idx in class_idx:\n",
    "        # Compute CAM for all samples in the batch\n",
    "        cam = np.dot(weight_softmax[idx], feature_conv_reshaped)\n",
    "        cam = cam.reshape(bz, h, w)\n",
    "\n",
    "        # Apply min-max normalization across all samples in the batch\n",
    "        cam_min = cam.min(axis=(1, 2), keepdims=True)\n",
    "        cam_max = cam.max(axis=(1, 2), keepdims=True)\n",
    "        cam_normalized = (cam - cam_min) / (cam_max - cam_min)\n",
    "\n",
    "        # Convert to uint8 and resize\n",
    "        cam_img = np.uint8(255 * cam_normalized)\n",
    "        output_cam.append(np.array([cv2.resize(img, size_upsample) for img in cam_img]))\n",
    "\n",
    "    # Stack the results for each class\n",
    "    return np.stack(output_cam, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_all_cams(feature_maps, classifier):\n",
    "    cams = []\n",
    "    for i in range(feature_maps.shape[0]):\n",
    "        video_cam = []\n",
    "        # Loop over all behaviors\n",
    "        for beh_idx in range(14):\n",
    "            beh_cam = []\n",
    "            # Get CAM for each frame\n",
    "            for f in range(feature_maps.shape[2]):\n",
    "\n",
    "                spatial_map = feature_maps[i, :, f, :, :].detach().numpy()\n",
    "                spatial_map = np.expand_dims(spatial_map, axis=0)\n",
    "                frame_cam = returnCAM_batch(\n",
    "                    spatial_map, classifier.weight.detach().cpu().numpy(), [beh_idx]\n",
    "                )\n",
    "                beh_cam.append(frame_cam)\n",
    "\n",
    "            # Normalize\n",
    "            beh_cam = np.stack(beh_cam)\n",
    "            beh_cam = beh_cam - np.min(beh_cam) / np.max(beh_cam)\n",
    "\n",
    "            # Convert to uint8\n",
    "            beh_cam = np.uint8(255 * beh_cam)\n",
    "            video_cam.append(beh_cam)\n",
    "\n",
    "        cams.append(np.stack(video_cam))\n",
    "\n",
    "    return np.stack(cams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_cams = return_all_cams(fg_feature_maps, bl_classifier)\n",
    "bg_cams = return_all_cams(bg_feature_maps, bg_classifier)\n",
    "\n",
    "fg_tensor_cams = torch.tensor(fg_cams).float()\n",
    "bg_tensor_cams = torch.tensor(bg_cams).float()\n",
    "\n",
    "fg_tensor_cams = torch.squeeze(fg_tensor_cams)\n",
    "bg_tensor_cams = torch.squeeze(bg_tensor_cams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_tensor_cams.shape, bg_tensor_cams.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_output_tensor = F.interpolate(\n",
    "    fg_tensor_cams.unsqueeze(0),\n",
    "    size=(360, 256, 256),\n",
    "    mode=\"trilinear\",\n",
    "    align_corners=False,\n",
    ")\n",
    "\n",
    "bg_output_tensor = F.interpolate(\n",
    "    bg_tensor_cams.unsqueeze(0),\n",
    "    size=(360, 256, 256),\n",
    "    mode=\"trilinear\",\n",
    "    align_corners=False,\n",
    ")\n",
    "\n",
    "# Convert tensor to uint8\n",
    "fg_output_tensor = fg_output_tensor.byte()\n",
    "bg_output_tensor = bg_output_tensor.byte()\n",
    "\n",
    "fg_output = fg_output_tensor.detach().numpy()\n",
    "bg_output = bg_output_tensor.detach().numpy()\n",
    "\n",
    "fg_output_tensor.shape, bg_output_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory if not exists\n",
    "save_dir = \"video_cams/train/fg\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "for bg_idx in range(batch_frames.shape[0]):\n",
    "    video_cam = fg_output[bg_idx]\n",
    "    bg = batch_frames[bg_idx]\n",
    "    for i in range(video_cam.shape[0]):\n",
    "        # create video as mp4\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        print(meta[\"fg_video_name\"][bg_idx])\n",
    "        out = cv2.VideoWriter(\n",
    "            os.path.join(\n",
    "                save_dir,\n",
    "                f\"{meta['fg_video_name'][bg_idx].split('.')[0]}_{bg_idx}_{i}.mp4\",\n",
    "            ),\n",
    "            fourcc,\n",
    "            24,\n",
    "            (256, 256),\n",
    "        )\n",
    "        results = []\n",
    "        for j in range(video_cam.shape[1]):\n",
    "            class_cam_frame = video_cam[i, j, ...]\n",
    "            class_cam_frame = np.uint8(255 * class_cam_frame)\n",
    "\n",
    "            heatmap = cv2.applyColorMap(class_cam_frame, cv2.COLORMAP_JET)\n",
    "            result = heatmap * 0.3 + bg[j] * 0.5\n",
    "            # convert to uint8\n",
    "            result = np.uint8(result)\n",
    "\n",
    "            out.write(result)\n",
    "            j += 24\n",
    "\n",
    "        out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory if not exists\n",
    "save_dir = \"video_cams/train/bg\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "for bg_idx in range(batch_frames.shape[0]):\n",
    "    video_cam = bg_output[bg_idx]\n",
    "    bg = batch_frames[bg_idx]\n",
    "    for i in range(video_cam.shape[0]):\n",
    "        # create video as mp4\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        out = cv2.VideoWriter(\n",
    "            os.path.join(\n",
    "                save_dir,\n",
    "                f\"{meta['fg_video_name'][bg_idx].split('.')[0]}_{bg_idx}_{i}.mp4\",\n",
    "            ),\n",
    "            fourcc,\n",
    "            24,\n",
    "            (256, 256),\n",
    "        )\n",
    "        results = []\n",
    "        for j in range(video_cam.shape[1]):\n",
    "            class_cam_frame = video_cam[i, j, ...]\n",
    "            class_cam_frame = np.uint8(255 * class_cam_frame)\n",
    "\n",
    "            heatmap = cv2.applyColorMap(class_cam_frame, cv2.COLORMAP_JET)\n",
    "            result = heatmap * 0.3 + bg[j] * 0.5\n",
    "            # convert to uint8\n",
    "            result = np.uint8(result)\n",
    "\n",
    "            out.write(result)\n",
    "            j += 24\n",
    "\n",
    "        out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "\n",
    "# Define the GlobalAvgPool2d class (as provided)\n",
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "\n",
    "    def forward(self, feature_map):\n",
    "        return F.adaptive_avg_pool2d(feature_map, 1).squeeze(-1).squeeze(-1)\n",
    "\n",
    "\n",
    "# Define the ImageClassifier class (as provided)\n",
    "class ImageClassifier(torch.nn.Module):\n",
    "    def __init__(self, P):\n",
    "        super(ImageClassifier, self).__init__()\n",
    "\n",
    "        self.arch = P[\"arch\"]\n",
    "        if P[\"dataset\"] == \"OPENIMAGES\":\n",
    "            feature_extractor = torchvision.models.resnet101(\n",
    "                pretrained=P[\"use_pretrained\"]\n",
    "            )\n",
    "        else:\n",
    "            feature_extractor = torchvision.models.resnet50(\n",
    "                pretrained=P[\"use_pretrained\"]\n",
    "            )\n",
    "        feature_extractor = torch.nn.Sequential(\n",
    "            *list(feature_extractor.children())[:-2]\n",
    "        )\n",
    "\n",
    "        if P[\"freeze_feature_extractor\"]:\n",
    "            for param in feature_extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            for param in feature_extractor.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.avgpool = GlobalAvgPool2d()\n",
    "        self.onebyone_conv = nn.Conv2d(P[\"feat_dim\"], P[\"num_classes\"], 1)\n",
    "        self.alpha = P[\"alpha\"]\n",
    "\n",
    "    def unfreeze_feature_extractor(self):\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.feature_extractor(x)\n",
    "        print(feats.shape)\n",
    "        CAM = self.onebyone_conv(feats)\n",
    "        print(CAM.shape)\n",
    "        CAM = torch.where(CAM > 0, CAM * self.alpha, CAM)  # BoostLU operation\n",
    "        logits = F.adaptive_avg_pool2d(CAM, 1).squeeze(-1).squeeze(-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy parameters\n",
    "P = {\n",
    "    \"arch\": \"resnet50\",\n",
    "    \"dataset\": \"IMAGENET\",  # or 'OPENIMAGES'\n",
    "    \"use_pretrained\": True,\n",
    "    \"freeze_feature_extractor\": False,\n",
    "    \"feat_dim\": 2048,  # For ResNet50/101, the feature dimension is 2048\n",
    "    \"num_classes\": 1000,  # Assuming ImageNet-like dataset\n",
    "    \"alpha\": 2.0,  # BoostLU parameter\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "model = ImageClassifier(P)\n",
    "\n",
    "# Create a dummy input tensor\n",
    "# Assuming a batch of 4 RGB images of size 224x224\n",
    "dummy_input = torch.randn(4, 3, 224, 224)\n",
    "\n",
    "# Forward pass\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoClassifier(nn.Module):\n",
    "    def __init__(self, P):\n",
    "        super(VideoClassifier, self).__init__()\n",
    "\n",
    "        self.arch = P[\"arch\"]\n",
    "\n",
    "        # Use 3D ResNet\n",
    "        if P[\"arch\"] == \"resnet3d50\":\n",
    "            base_model = torchvision.models.video.r3d_18(pretrained=P[\"use_pretrained\"])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported architecture: {P['arch']}\")\n",
    "\n",
    "        # Remove the last fully connected layer and global average pooling\n",
    "        self.feature_extractor = nn.Sequential(*list(base_model.children())[:-2])\n",
    "\n",
    "        if P[\"freeze_feature_extractor\"]:\n",
    "            for param in self.feature_extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            for param in self.feature_extractor.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        self.onebyone_conv = nn.Conv3d(P[\"feat_dim\"], P[\"num_classes\"], kernel_size=1)\n",
    "        self.alpha = P[\"alpha\"]\n",
    "\n",
    "    def unfreeze_feature_extractor(self):\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, channels, time, height, width)\n",
    "        features = self.feature_extractor(x)\n",
    "        print(features.shape)\n",
    "        CAM = self.onebyone_conv(features)\n",
    "        print(CAM.shape)\n",
    "        CAM = torch.where(CAM > 0, CAM * self.alpha, CAM)  # BoostLU operation\n",
    "        print(CAM.shape)\n",
    "        logits = F.adaptive_avg_pool3d(CAM, 1).squeeze(-1).squeeze(-1).squeeze(-1)\n",
    "        return logits, CAM, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy parameters\n",
    "P = {\n",
    "    \"arch\": \"resnet3d50\",  # or 'resnet3d101'\n",
    "    \"use_pretrained\": True,\n",
    "    \"freeze_feature_extractor\": False,\n",
    "    \"feat_dim\": 512,  # For 3D ResNet50/101, the feature dimension is 2048\n",
    "    \"num_classes\": 400,  # Assuming Kinetics-400 dataset\n",
    "    \"alpha\": 2.0,  # BoostLU parameter\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "model = VideoClassifier(P)\n",
    "model.eval()\n",
    "\n",
    "# Create a dummy input tensor\n",
    "# Assuming a batch of 2 videos, each with 16 frames of 3 channels and 224x224 resolution\n",
    "dummy_input = torch.randn(2, 3, 16, 224, 224)\n",
    "\n",
    "# Forward pass\n",
    "logits, CAM, features = model(dummy_input)\n",
    "\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Feature shape: {features.shape}\")\n",
    "print(f\"CAM shape: {CAM.shape}\")\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "# Detailed analysis of the feature extractor\n",
    "# def print_output_shape(name):\n",
    "#     def hook(model, input, output):\n",
    "#         print(f\"{name} output shape: {output.shape}\")\n",
    "#     return hook\n",
    "\n",
    "# Attach hooks to each layer of the feature extractor\n",
    "# for name, layer in model.feature_extractor.named_children():\n",
    "#     layer.register_forward_hook(print_output_shape(name))\n",
    "\n",
    "# Another forward pass to print intermediate shapes\n",
    "# _ = model(dummy_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slowfast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
